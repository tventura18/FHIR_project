{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "128eb128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "527dced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----DB config, fetch and insert\n",
    "import psycopg2\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "#from tqdm import tqdm\n",
    "from datetime import datetime, timezone\n",
    "#import datetime  # module\n",
    "#from datetime import datetime as dt  # class, aliased to avoid conflict\n",
    "#from datetime import datetime as tz\n",
    "#import datetime\n",
    "import pprint\n",
    "import dateutil.parser\n",
    "import json\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "# Path to your service account JSON key\n",
    "#key_path = \"/Users/toniventura/keys/bq_key.json\" \n",
    "key_path = \"C:\\\\Users\\\\tonim\\\\keys\\\\bq_key.json\"\n",
    "\n",
    "# Create credentials and BigQuery client\n",
    "#credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "\n",
    "# Postgres config\n",
    "PG_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    #\"database\": \"fhir\",\n",
    "    \"database\" : \"FHIR_staging\",\n",
    "    #\"user\": \"toniventura\",\n",
    "    \"user\": \"postgres\",\n",
    "    #\"password\": \"fhir_project\"\n",
    "    \"password\": \"new_password\"\n",
    "}\n",
    "#records =[]\n",
    "\n",
    "# BigQuery config\n",
    "#BQ_PROJECT = \"your-gcp-project\"\n",
    "BQ_PROJECT = \"fhir-synthea-data\"\n",
    "#BQ_DATASET = \"fhir_curated\"\n",
    "BQ_DATASET = \"fhir_curated\"\n",
    "#client = bigquery.Client(project=BQ_PROJECT)\n",
    "client = bigquery.Client(project=\"fhir-synthea-data\", credentials=credentials)\n",
    "#client = bigquery.Client(project=BQ_PROJECT, credentials=credentials)\n",
    "dataset_ref = bigquery.Dataset(f\"{BQ_PROJECT}.{BQ_DATASET}\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=\"etl_eobs.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\"\n",
    ")\n",
    "\n",
    "# Helper: fetch staged data\n",
    "def fetch_staged_data(table, batch_size=10000):\n",
    "    try:\n",
    "        conn = psycopg2.connect(**PG_CONFIG)\n",
    "        cur = conn.cursor()\n",
    "        #cur.execute(f\"SELECT * FROM fhir_staging.{table}\")\n",
    "        #cur.execute(\"SELECT * FROM fhir_staging.patients_fhir_raw LIMIT 5;\")\n",
    "        cur.execute(f\"SELECT * FROM fhir_staging_sample.{table}\")\n",
    "        #cur.execute(f\"SELECT * FROM fhir_staging_sample.{table};\")\n",
    "        while True:\n",
    "            rows = cur.fetchmany(batch_size)\n",
    "            logging.info(f\"rows: {rows}\")\n",
    "            if not rows:\n",
    "                break\n",
    "            yield rows\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Postgres connection or query failed: {e}\")\n",
    "        \n",
    "\n",
    "\n",
    "# Helper: insert dataframe into BigQuery\n",
    "def insert_to_bq(df, table_name):\n",
    "    table_id = f\"{BQ_PROJECT}.{BQ_DATASET}.{table_name}\"\n",
    "    job = client.load_table_from_dataframe(df, table_id)\n",
    "    job.result()  # wait for completion\n",
    "\n",
    "\n",
    "def safe_bq_timestamp(dt):\n",
    "    if not dt:\n",
    "        return None\n",
    "    return dt.astimezone(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aca5e82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main ETL loop\n",
    "def etl_EOBs():\n",
    "    try:\n",
    "         for batch in fetch_staged_data(\"explanationofbenefits_fhir_raw\"):\n",
    "            #table_id = \"fhir-synthea-data.fhir_curated.eob\"\n",
    "            table_id = \"fhir-synthea-data.fhir_curated_sample.eobs\" \n",
    "            logging.info(f\"Processing batch for table: {table_id}\")    \n",
    "            job_config = bigquery.LoadJobConfig(\n",
    "                schema=[\n",
    "                    bigquery.SchemaField(\"eob_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "                    #bigquery.SchemaField(\"patient_id\", \"STRING\"),\n",
    "                    #bigquery.SchemaField(\"organization_id\", \"STRING\"),\n",
    "                    #bigquery.SchemaField(\"encounter_class\", \"STRING\"),\n",
    "                    #bigquery.SchemaField(\"encounter_status\", \"STRING\"),\n",
    "                    #bigquery.SchemaField(\"encounter_type_code\", \"STRING\"),\n",
    "                    #bigquery.SchemaField(\"encounter_type_display\", \"STRING\"),\n",
    "                    #bigquery.SchemaField(\"start_datetime\", \"TIMESTAMP\"),\n",
    "                    #bigquery.SchemaField(\"end_datetime\", \"TIMESTAMP\"),\n",
    "                    bigquery.SchemaField(\"load_timestamp\", \"TIMESTAMP\", mode=\"REQUIRED\")\n",
    "                ]\n",
    "            )\n",
    "    except:\n",
    "        logging.error(f\"Error in EOB's ETL: {e}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"Starting EOB ETL...\")\n",
    "    #print(\"main\")\n",
    "    etl_EOBs()\n",
    "    logging.info(\"Finished EOB ETL\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13",
   "language": "python",
   "name": "python313"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
