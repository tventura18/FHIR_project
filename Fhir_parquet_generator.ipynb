{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f86a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "\n",
    "# Optional: FHIR validation\n",
    "USE_VALIDATION = False\n",
    "if USE_VALIDATION:\n",
    "    from fhir.resources.patient import Patient\n",
    "\n",
    "DATA_DIR = Path(\"C:\\\\Users\\\\tonim\\\\Downloads\\\\output\\\\fhir\\\\Patients\")\n",
    "OUTPUT_FILE = \"C:\\\\Users\\\\tonim\\\\Downloads\\\\output\\\\parquet\\\\patients.parquet\"\n",
    "    \n",
    "\n",
    "def process_file(file):\n",
    "    try:\n",
    "        # your parsing code\n",
    "        return parsed_rows\n",
    "    except Exception as e:\n",
    "        print(f\"X Error in {file}: {e}\")\n",
    "        return None\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=1) as executor:\n",
    "    results = list(executor.map(process_file, batch_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e22ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "\n",
    "# Load a single FHIR JSON\n",
    "with open(\"C:\\\\Users\\\\tonim\\\\Downloads\\\\output\\\\fhir\\\\Patients\\\\Floyd420_Streich926_42f4db2f-b049-c9a1-a961-7a944ea72e48.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Flatten JSON\n",
    "df = pd.json_normalize(data)\n",
    "\n",
    "#print(df.head())\n",
    "df.to_parquet(\n",
    "    \"C:\\\\Users\\\\tonim\\\\Downloads\\\\output\\\\parquet\\\\patient.parquet\",\n",
    "    engine=\"fastparquet\",\n",
    "    index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8376e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "INPUT_DIR = \"C:\\\\Users\\\\tonim\\\\Downloads\\\\output\\\\fhir\\\\Patients\"\n",
    "OUTPUT_DIR = \"C:\\\\Users\\\\tonim\\\\Downloads\\\\output\\\\parquet\"\n",
    "BATCH_SIZE = 500\n",
    "PARQUET_FILE = os.path.join(OUTPUT_DIR, \"patients.parquet\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def process_file(filepath):\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Example flatten (customize per schema)\n",
    "        record = {\n",
    "            \"id\": data.get(\"id\"),\n",
    "            \"name\": data.get(\"name\", {}).get(\"given\", [None])[0],\n",
    "            \"gender\": data.get(\"gender\"),\n",
    "            \"birthDate\": data.get(\"birthDate\")\n",
    "        }\n",
    "        return record\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Failed: {filepath} ({e})\")\n",
    "        return None\n",
    "\n",
    "def process_batch(batch_files):\n",
    "    rows = []\n",
    "    with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "        futures = [executor.submit(process_file, f) for f in batch_files]\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                rows.append(result)\n",
    "\n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows)\n",
    "        table = pa.Table.from_pandas(df)\n",
    "\n",
    "        if not os.path.exists(PARQUET_FILE):\n",
    "            pq.write_table(table, PARQUET_FILE)\n",
    "        else:\n",
    "            pq.write_table(table, PARQUET_FILE, append=True)\n",
    "\n",
    "def main():\n",
    "    all_files = [os.path.join(INPUT_DIR, f) for f in os.listdir(INPUT_DIR) if f.endswith(\".json\")]\n",
    "\n",
    "    for i in range(0, len(all_files), BATCH_SIZE):\n",
    "        batch_files = all_files[i:i + BATCH_SIZE]\n",
    "        print(f\"â–¶ Processing batch {i // BATCH_SIZE + 1}: {len(batch_files)} files\")\n",
    "        process_batch(batch_files)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6936136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Global logging configuration (optional, you can set handlers per logger too)\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Set to DEBUG to capture all debug messages\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Create individual loggers\n",
    "patient_logger = logging.getLogger('parser.patient')\n",
    "encounter_logger = logging.getLogger('parser.encounter')\n",
    "document_logger = logging.getLogger('parser.document')\n",
    "care_plan_logger = logging.getLogger('parser.care_plan')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b427404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import logging\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "import logging\n",
    "import json\n",
    "\n",
    "####-----------------------------------09/03/2025------------------------\n",
    "# Parsers ---> parqut\n",
    "\n",
    "# Create a dedicated logger for claims parsing\n",
    "#claims_logger = logging.getLogger(\"claims_parser\")\n",
    "#claims_logger.setLevel(logging.DEBUG)  # or INFO if you prefer\n",
    "\n",
    "# Create a file handler that writes to a dedicated file\n",
    "#file_handler = logging.FileHandler(\"claims_parsing.log\", mode=\"w\", encoding=\"utf-8\")\n",
    "#file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Optional: add a formatter\n",
    "#formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "#file_handler.setFormatter(formatter)\n",
    "\n",
    "# Add handler to the logger\n",
    "#claims_logger.addHandler(file_handler)\n",
    "#---------------------------------------------\n",
    "#Logger\n",
    "#--------------------------------------------\n",
    "\n",
    "def get_parser_logger(name, log_file, level=logging.DEBUG):\n",
    "    \"\"\"Create a dedicated logger for a parser method.\"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    \n",
    "    # Avoid adding multiple handlers if logger already exists\n",
    "    if not logger.handlers:\n",
    "        file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')\n",
    "        file_handler.setLevel(level)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# -------------------------\n",
    "# Helper Functions\n",
    "# -------------------------\n",
    "\n",
    "def extract_codings(resource_field):\n",
    "    \"\"\"\n",
    "    Given a FHIR resource field that may have a 'coding' list,\n",
    "    return a list of dicts with 'system' and 'code'.\n",
    "    \"\"\"\n",
    "    codings = resource_field.get(\"coding\", [])\n",
    "    result = []\n",
    "    for coding in codings:\n",
    "        system = coding.get(\"system\")\n",
    "        code = coding.get(\"code\")\n",
    "        result.append({\"system\": system, \"code\": code})\n",
    "    return result\n",
    "\n",
    "def extract_diagnoses_from_claim(resource):\n",
    "    \"\"\"\n",
    "    Extract diagnoses directly from claim resource.\n",
    "    Returns a list of dicts with sequence, reference, and code.\n",
    "    \"\"\"\n",
    "    diagnoses = []\n",
    "\n",
    "    for diag in resource.get(\"diagnosis\", []):\n",
    "        sequence = diag.get(\"sequence\")\n",
    "        diagnosis_ref = diag.get(\"diagnosisReference\", {}).get(\"reference\")\n",
    "        codeable_concept = diag.get(\"diagnosisCodeableConcept\", {}).get(\"coding\", [{}])[0].get(\"code\")\n",
    "        diagnoses.append({\n",
    "            \"sequence\": sequence,\n",
    "            \"diagnosisReference\": diagnosis_ref,\n",
    "            \"diagnosisCodeableConcept\": codeable_concept\n",
    "        })\n",
    "\n",
    "    return diagnoses\n",
    "\n",
    "#---getting diagnostic and insurance info from the sequences\n",
    "def combine_claim_info_by_sequence(resource):\n",
    "    \"\"\"\n",
    "    Combine supportingInfo, diagnoses, and insurance by sequence.\n",
    "    Returns a dictionary keyed by sequence with all associated info.\n",
    "    \"\"\"\n",
    "    combined = {}\n",
    "\n",
    "    # Start with supporting info\n",
    "    info_list, _ = extract_supporting_info_combined(resource)\n",
    "    for entry in info_list:\n",
    "        seq = entry[\"sequence\"]\n",
    "        combined.setdefault(seq, {})\n",
    "        combined[seq].setdefault(\"supporting_info\", []).append(entry)\n",
    "\n",
    "    # Add diagnoses\n",
    "    for diag in extract_diagnoses_from_claim(resource):\n",
    "        seq = diag[\"sequence\"]\n",
    "        combined.setdefault(seq, {})\n",
    "        combined[seq].setdefault(\"diagnoses\", []).append({\n",
    "            \"diagnosisReference\": diag.get(\"diagnosisReference\"),\n",
    "            \"diagnosisCodeableConcept\": diag.get(\"diagnosisCodeableConcept\")\n",
    "        })\n",
    "\n",
    "    # Add insurance\n",
    "    for ins in extract_insurance_from_claim(resource):\n",
    "        seq = ins[\"sequence\"]\n",
    "        combined.setdefault(seq, {})\n",
    "        combined[seq].setdefault(\"insurance\", []).append({\n",
    "            \"focal\": ins.get(\"focal\"),\n",
    "            \"coverage\": ins.get(\"coverage\")\n",
    "        })\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "def extract_insurance_from_claim(resource):\n",
    "    \"\"\"\n",
    "    Extract insurance entries directly from claim resource.\n",
    "    Returns a list of dicts with sequence, focal, and coverage.\n",
    "    \"\"\"\n",
    "    insurance_list = []\n",
    "\n",
    "    for ins in resource.get(\"insurance\", []):\n",
    "        insurance_list.append({\n",
    "            \"sequence\": ins.get(\"sequence\"),\n",
    "            \"focal\": ins.get(\"focal\"),\n",
    "            \"coverage\": ins.get(\"coverage\", {}).get(\"reference\") \n",
    "                        or ins.get(\"coverage\", {}).get(\"display\")\n",
    "        })\n",
    "\n",
    "    return insurance_list\n",
    "\n",
    "\n",
    "def extract_supporting_info_combined(resource):\n",
    "    \"\"\"\n",
    "    Extract supportingInfo entries from a FHIR resource.\n",
    "    Returns both:\n",
    "      - a list preserving original order\n",
    "      - a dict grouped by sequence\n",
    "    \"\"\"\n",
    "    supporting_info = resource.get(\"supportingInfo\", [])\n",
    "    info_list = []\n",
    "    info_by_sequence = {}\n",
    "\n",
    "    for info in supporting_info:\n",
    "        seq = info.get(\"sequence\")\n",
    "        if seq is None:\n",
    "            continue\n",
    "\n",
    "        category = []\n",
    "        for coding in info.get(\"category\", {}).get(\"coding\", []):\n",
    "            category.append({\n",
    "                \"system\": coding.get(\"system\"),\n",
    "                \"code\": coding.get(\"code\"),\n",
    "                \"display\": coding.get(\"display\")\n",
    "            })\n",
    "\n",
    "        entry = {\n",
    "            \"sequence\": seq,\n",
    "            \"category\": category,\n",
    "            \"valueString\": info.get(\"valueString\"),\n",
    "            \"valueBoolean\": info.get(\"valueBoolean\"),\n",
    "            \"valueQuantity\": info.get(\"valueQuantity\"),\n",
    "            \"valueAttachment\": info.get(\"valueAttachment\"),\n",
    "            \"timingDate\": info.get(\"timingDate\"),\n",
    "            \"timingPeriod\": info.get(\"timingPeriod\")\n",
    "        }\n",
    "\n",
    "        info_list.append(entry)\n",
    "        info_by_sequence.setdefault(seq, []).append(entry)\n",
    "\n",
    "    return info_list, info_by_sequence\n",
    "\n",
    "def extract_EOB_info(resource):\n",
    "    \n",
    "    contained_info = resource.get(\"contained\", [])\n",
    "    \"\"\"for contained in contained_info:\n",
    "        type = contained.get(\"resourceType\")\n",
    "        id = contained.get(\"id\")\n",
    "        status = contained.get(\"status\")\n",
    "        intent = contained.get(\"intent\")\n",
    "        reference = contained.get(\"subject\").get(\"reference\")\n",
    "        requester = contained.get(\"requester\").get(\"reference\")\n",
    "        performer = contained.get(\"performer\").get(\"performer\")\n",
    "        EOB_extracted_info. append({\n",
    "            \"type\": type,\n",
    "            \"id\": id,\n",
    "            \"status\": status,\n",
    "            \"intent\": intent,\n",
    "            \"reference\": reference,\n",
    "            \"requester\":requester,\n",
    "            \"performer\": performer    \n",
    "        })\"\"\"\n",
    "    return contained_info\n",
    "\n",
    "\n",
    "    #resource[\"diagnosis_parsed\"] = diagnoses    \n",
    "\n",
    "def extract_supporting_info_combined(resource):\n",
    "    \"\"\"\n",
    "    Extract supportingInfo entries from a FHIR resource.\n",
    "    Returns both:\n",
    "      - a list preserving original order\n",
    "      - a dict grouped by sequence\n",
    "    \"\"\"\n",
    "    supporting_info = resource.get(\"supportingInfo\", [])\n",
    "    info_list = []\n",
    "    info_by_sequence = {}\n",
    "\n",
    "    for info in supporting_info:\n",
    "        seq = info.get(\"sequence\")\n",
    "        if seq is None:\n",
    "            continue  # skip entries without sequence\n",
    "\n",
    "        # Extract all codings under category\n",
    "        category = []\n",
    "        for coding in info.get(\"category\", {}).get(\"coding\", []):\n",
    "            category.append({\n",
    "                \"system\": coding.get(\"system\"),\n",
    "                \"code\": coding.get(\"code\"),\n",
    "                \"display\": coding.get(\"display\")\n",
    "            })\n",
    "\n",
    "        entry = {\n",
    "            \"sequence\": seq,\n",
    "            \"category\": category,\n",
    "            \"valueString\": info.get(\"valueString\"),\n",
    "            \"valueBoolean\": info.get(\"valueBoolean\"),\n",
    "            \"valueQuantity\": info.get(\"valueQuantity\"),\n",
    "            \"valueAttachment\": info.get(\"valueAttachment\"),\n",
    "            \"timingDate\": info.get(\"timingDate\"),\n",
    "            \"timingPeriod\": info.get(\"timingPeriod\")\n",
    "        }\n",
    "\n",
    "        # Add to list (preserves order)\n",
    "        info_list.append(entry)\n",
    "\n",
    "        # Add to dict (grouped by sequence)\n",
    "        if seq not in info_by_sequence:\n",
    "            info_by_sequence[seq] = []\n",
    "        info_by_sequence[seq].append(entry)\n",
    "\n",
    "    return info_list, info_by_sequence\n",
    "\n",
    "\n",
    "def safe_get(resource, *keys, default=None):\n",
    "    \"\"\"Safely navigate nested dicts and lists.\"\"\"\n",
    "    try:\n",
    "        for key in keys:\n",
    "            if isinstance(resource, list) and isinstance(key, int):\n",
    "                resource = resource[key]\n",
    "            else:\n",
    "                resource = resource[key]\n",
    "        return resource\n",
    "    except (KeyError, IndexError, TypeError):\n",
    "        return default\n",
    "\n",
    "def safe_get_first(lst, default=None):\n",
    "    \"\"\"Return first element of a list if it exists.\"\"\"\n",
    "    if isinstance(lst, list) and lst:\n",
    "        return lst[0]\n",
    "    return default\n",
    "\n",
    "#def safe_get_first_coding(resource, field):\n",
    "    \"\"\"Return the first coding dict from a field.\"\"\"\n",
    "#    coding = safe_get(resource, field, 0, \"coding\", 0, default={})\n",
    "#    logging.info(f\"coding: {coding}\")\n",
    "#    return coding if coding else {}\n",
    "\n",
    "def get_first_coding(resource, field):\n",
    "    \"\"\"Safely extract first coding (code, system, display) from a FHIR element.\"\"\"\n",
    "    if field in resource and isinstance(resource[field], dict):\n",
    "        coding_list = resource[field].get(\"coding\", [])\n",
    "        if coding_list and isinstance(coding_list, list):\n",
    "            return coding_list[0]  # first coding dict\n",
    "    return {}\n",
    "\n",
    "def extract_identifier(resource, code_to_find):\n",
    "    \"\"\"Return identifier value for a given code (SS, DL, PPN).\"\"\"\n",
    "    for ident in resource.get(\"identifier\", []):\n",
    "        for c in ident.get(\"type\", {}).get(\"coding\", []):\n",
    "            if c.get(\"code\") == code_to_find:\n",
    "                return ident.get(\"value\")\n",
    "    return None\n",
    "\n",
    "def extract_extensions(resource, url_to_find):\n",
    "    \"\"\"Extract the 'valueString' from a specific extension url.\"\"\"\n",
    "    for ext in resource.get(\"extension\", []):\n",
    "        if ext.get(\"url\") == url_to_find:\n",
    "            for sub in ext.get(\"extension\", []):\n",
    "                if sub.get(\"url\") == \"text\":\n",
    "                    return sub.get(\"valueString\")\n",
    "    return None\n",
    "\n",
    "# -------------------------\n",
    "# Parsers\n",
    "# -------------------------\n",
    "def parse_patient_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if \"entry\" not in data:\n",
    "        return []\n",
    "\n",
    "    patients = []\n",
    "    for entry in data[\"entry\"]:\n",
    "        resource = entry.get(\"resource\", {})\n",
    "        if resource.get(\"resourceType\") != \"Patient\":\n",
    "            continue\n",
    "        \n",
    "        names = resource.get(\"name\",[])\n",
    "        name = names[0] if names else{}\n",
    "        #name = resource.get(\"name\", []), {})\n",
    "        given_name = safe_get(name.get(\"given\", 0))\n",
    "        last_name = name.get(\"family\")\n",
    "        prefix = safe_get(name.get(\"prefix\", 0))\n",
    "\n",
    "        extensions = safe_get(resource, \"extension\", default=[])\n",
    "        birth_place = extensions[0].get(\"valueAddress\", {}) if extensions else {}\n",
    "\n",
    "        patients.append({\n",
    "            \"id\": resource.get(\"id\"),\n",
    "            \"first_name\": given_name,\n",
    "            \"last_name\": last_name,\n",
    "            \"prefix\": prefix,\n",
    "            \"gender\": resource.get(\"gender\"),\n",
    "            \"birth_date\": resource.get(\"birthDate\"),\n",
    "            \"deceased_date_time\": resource.get(\"deceasedDateTime\"),\n",
    "            \"ssn\": extract_identifier(resource, \"SS\"),\n",
    "            \"driversLicense\": extract_identifier(resource, \"DL\"),\n",
    "            \"Passport\": extract_identifier(resource, \"PPN\"),\n",
    "            \"maritalStatus\": safe_get(resource, \"maritalStatus\", \"text\") or safe_get_first(resource.get(\"maritalStatus\", {}).get(\"coding\", []), {}).get(\"display\"),\n",
    "            \"race\": extract_extensions(resource, \"http://hl7.org/fhir/us/core/StructureDefinition/us-core-race\"),\n",
    "            \"ethnicity\": extract_extensions(resource, \"http://hl7.org/fhir/us/core/StructureDefinition/us-core-ethnicity\"),\n",
    "            \"birth_place\": birth_place\n",
    "        })\n",
    "    return patients\n",
    "\n",
    "def parse_encounter_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if \"entry\" not in data:\n",
    "        return []\n",
    "\n",
    "    encounters = []\n",
    "    for entry in data[\"entry\"]:\n",
    "        resource = entry.get(\"resource\", {})\n",
    "        if resource.get(\"resourceType\") != \"Encounter\":\n",
    "            continue\n",
    "\n",
    "        coding = get_first_coding(resource, \"type\")\n",
    "        encounters.append({\n",
    "            \"id\": resource.get(\"id\"),\n",
    "            \"patient_ref\": safe_get(resource, \"subject\", \"reference\"),\n",
    "            \"status\": resource.get(\"status\"),\n",
    "            \"code\": coding.get(\"code\"),\n",
    "            \"description\": coding.get(\"display\"),\n",
    "            \"start_date_time\": safe_get(resource, \"period\", \"start\"),\n",
    "            \"end_date_time\": safe_get(resource, \"period\", \"end\")\n",
    "        })\n",
    "    return encounters\n",
    "\n",
    "def insert_conditions(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if \"entry\" not in data:\n",
    "        return []\n",
    "\n",
    "    rows = []\n",
    "    for entry in data[\"entry\"]:\n",
    "        resource = entry.get(\"resource\", {})\n",
    "        if resource.get(\"resourceType\") != \"Condition\":\n",
    "            continue\n",
    "\n",
    "        coding = get_first_coding(resource, \"code\")\n",
    "        category_coding = get_first_coding(resource, \"category\") if resource.get(\"category\") else {}\n",
    "\n",
    "        rows.append({\n",
    "            \"resource_type\": \"Condition\",\n",
    "            \"resource\": json.dumps(resource),\n",
    "            \"condition_id\": resource.get(\"id\"),\n",
    "            \"patient_ref\": safe_get(resource, \"subject\", \"reference\"),\n",
    "            \"encounter_ref\": safe_get(resource, \"encounter\", \"reference\"),\n",
    "            \"code\": coding.get(\"code\"),\n",
    "            \"description\": coding.get(\"display\"),\n",
    "            \"category\": category_coding.get(\"code\"),\n",
    "            \"onset_date\": resource.get(\"onsetDateTime\")\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "def parse_observations(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if \"entry\" not in data:\n",
    "        return []\n",
    "\n",
    "    observations = []\n",
    "    for entry in data[\"entry\"]:\n",
    "        resource = entry.get(\"resource\", {})\n",
    "        if resource.get(\"resourceType\") != \"Observation\":\n",
    "            continue\n",
    "\n",
    "        coding = get_first_coding(resource, \"code\")\n",
    "        observations.append({\n",
    "            \"observation_id\": resource.get(\"id\"),\n",
    "            \"code\": coding.get(\"code\"),\n",
    "            \"system\": coding.get(\"system\"),\n",
    "            \"description\": coding.get(\"display\"),\n",
    "            \"patient_ref\": safe_get(resource, \"subject\", \"reference\"),\n",
    "            \"encounter_ref\": safe_get(resource, \"encounter\", \"reference\")\n",
    "        })\n",
    "    return observations\n",
    "\n",
    "def parse_diagnostic_reports(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if \"entry\" not in data:\n",
    "        return []\n",
    "\n",
    "    reports = []\n",
    "    for entry in data[\"entry\"]:\n",
    "        resource = entry.get(\"resource\", {})\n",
    "        if resource.get(\"resourceType\") != \"DiagnosticReport\":\n",
    "            continue\n",
    "        diagnostic_id = resource.get(\"id\")\n",
    "        #logging.info(f\"diagnostic_id: {diagnostic_id}\")\n",
    "        #coding = safe_get_first_coding(resource, \"code\")\n",
    "        coding = get_first_coding(resource, \"code\")\n",
    "        #logging.info(f\"coding: {coding}\")\n",
    "        code = coding.get(\"code\")\n",
    "        #logging.info(f\"coding: {code}\")\n",
    "        #results_list = [r.get(\"display\") for r in resource.get(\"result\", [])]\n",
    "        results_list = []\n",
    "        if \"result\" in resource:\n",
    "            for r in resource[\"result\"]:\n",
    "                ref = r.get(\"reference\")  # e.g. \"Observation/12345\"\n",
    "                display = r.get(\"display\")\n",
    "                #logging.info(f\"display: {display}\")\n",
    "                \n",
    "                results_list.append({\n",
    "                    \"reference\" : ref,\n",
    "                    \"display\": display\n",
    "                })\n",
    "                \n",
    "        #logging.info(f\"result_list: {results_list}\")\n",
    "        system = coding.get(\"system\")\n",
    "        #logging.info(f\"system: {system}\")\n",
    "        description = coding.get(\"display\")\n",
    "        #logging.info(f\"description: {description}\")\n",
    "        patient_ref = safe_get(resource, \"subject\", \"reference\")\n",
    "        #logging.info(f\"patient_ref: {patient_ref}\")\n",
    "        encounter_ref =  safe_get(resource, \"encounter\", \"reference\")\n",
    "        #logging.info(f\"encounter:{encounter_ref}\")\n",
    "        effective_date_time = resource.get(\"effectiveDateTime\")\n",
    "        #logging.info(f\"effective_date_time: {effective_date_time}\")\n",
    "\n",
    "\n",
    "        reports.append({\n",
    "            \"diagnostic_id\": diagnostic_id,\n",
    "            \"code\": code,\n",
    "            \"system\": system,\n",
    "            \"description\": description,\n",
    "            \"patient_ref\": patient_ref,\n",
    "            \"encounter_ref\": encounter_ref,\n",
    "            \"encounter_date_time\": effective_date_time,\n",
    "            \"results\": json.dumps(results_list)\n",
    "        })\n",
    "    return reports\n",
    "\n",
    "'''def parse_document_reference(file_path):\n",
    "    #print(\"inside document_reference\")\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        if \"entry\" not in data:\n",
    "                return None\n",
    "        \n",
    "        document_references=[]\n",
    "        for entry in data[\"entry\"]:\n",
    "            resource = entry.get(\"resource\", {})\n",
    "            if resource.get(\"resourceType\") == \"DocumentReference\":\n",
    "                logging.info(\"----insert document reference.\")\n",
    "                document_id = resource.get(\"id\") \n",
    "                logging.info(\"------Document Reference--------\")\n",
    "                logging.info(f\"document_id: {document_id}\") \n",
    "                status = resource.get(\"status\")\n",
    "                patient_ref = safe_get(resource, \"subject\", \"reference\")\n",
    "                logging.info(f\"patient_ref: {patient_ref}\")\n",
    "                #encounter_ref = safe_get(resource, \"context\", \"encounter\")\n",
    "                #logging.info(f\"encounter_ref:{encounter_ref}\")\n",
    "                #coding = get_first_coding(resource, \"code\")\n",
    "\n",
    "                 # context.encounter can be an array of references\n",
    "                context = resource.get(\"context\", {})\n",
    "\n",
    "                encounter_refs = safe_get(resource, \"context\", \"encounter\") or []\n",
    "                encounters_info = []\n",
    "                for enc in encounter_refs:\n",
    "                    if encounter_refs:\n",
    "                        encounters_info.append({\n",
    "                            \"encounter_ref\": enc.get(\"reference\"),\n",
    "                            \"encounter_display\": enc.get(\"display\")\n",
    "                        })\n",
    "                \n",
    "                date_time = resource.get(\"date\")\n",
    "                logging.info(f\"date_time: {date_time}\")\n",
    "\n",
    "                codings = resource.get(\"type\", {}).get(\"coding\", [])\n",
    "                code_info =[]\n",
    "                #logging.info(f\"coding: {codings}\")\n",
    "                for coding in codings:\n",
    "                    code = coding.get(\"code\")\n",
    "                    #codes.append(code)\n",
    "                    logging.info(f\"coding: {code}\")\n",
    "                    \n",
    "                    system = coding.get(\"system\")\n",
    "                    #systems.append(system)\n",
    "                    logging.info(f\"system: {system}\")\n",
    "                    description = coding.get(\"display\")\n",
    "                    #descriptions.append(description)\n",
    "                    logging.info(f\"description: {description}\")\n",
    "\n",
    "                    code_info.append({\n",
    "                        \"code\": code,\n",
    "                        \"system\" : system,\n",
    "                        \"description\" : description    \n",
    "                    })\n",
    "                author_info = []\n",
    "                authors = resource.get(\"author\", [])\n",
    "                for author in authors:\n",
    "                    author_ref = author.get(\"reference\")\n",
    "                    logging.info(f\"author_ref: {author_ref}\")\n",
    "                    author_display = author.get(\"display\")\n",
    "                    logging.info(f\"author_name: {author_display}\")\n",
    "                    author_info.append({\n",
    "                        \"author_ref\": author_ref,\n",
    "                        \"author_display\" : author_display         \n",
    "                    })\n",
    "                \n",
    "                custodian_info = []\n",
    "                custodian = resource.get(\"custodian\",{})\n",
    "                custodian_reference = custodian.get(\"reference\")\n",
    "                logging.info(f\"custodian_reference: {custodian_reference}\")\n",
    "                custodian_display = custodian.get(\"display\")\n",
    "                logging.info(f\"custodian_display: {custodian_display}\")\n",
    "                custodian_info.append({\n",
    "                    \"custodian_reference\" : custodian_reference,\n",
    "                    \"custodian_display\"  : custodian_display\n",
    "                })\n",
    "\n",
    "                start_date_time = resource.get(\"context\", {}).get(\"period\", {}).get(\"start\")\n",
    "                logging.info(f\"start_date_time: {start_date_time}\")\n",
    "                end_date_time = resource.get(\"context\", {}).get(\"period\", {}).get(\"end\")\n",
    "                logging.info(f\"end_date_time: {end_date_time}\")\n",
    "\n",
    "                document_references.append({\n",
    "                    \"document_id\": document_id,\n",
    "                    \"status\": status,\n",
    "                    \"patient_ref\": patient_ref,\n",
    "                    \"encounter_ref\" : json.dumps(encounters_info),\n",
    "                    \"date_time\" : date_time, \n",
    "                    \"code_info\" : json.dumps(code_info),\n",
    "                    \"author_info\" : json.dumps(author_info),\n",
    "                    \"custodian\" : json.dumps(custodian_info),\n",
    "                    \"start_date_time\" : start_date_time,\n",
    "                    \"end_date_time\" : end_date_time\n",
    "                })      \n",
    "\n",
    "        return document_references''' \n",
    "\n",
    "def parse_document_reference(file_path):\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        if \"entry\" not in data:\n",
    "            return None\n",
    "\n",
    "        document_references = []\n",
    "\n",
    "        for entry in data[\"entry\"]:\n",
    "            resource = entry.get(\"resource\", {})\n",
    "            if resource.get(\"resourceType\") != \"DocumentReference\":\n",
    "                continue\n",
    "\n",
    "            #logging.info(\"----insert document reference.\")\n",
    "            document_id = resource.get(\"id\")\n",
    "            #logging.info(f\"document_id: {document_id}\")\n",
    "\n",
    "            status = resource.get(\"status\")\n",
    "            patient_ref = safe_get(resource, \"subject\", \"reference\")\n",
    "            #logging.info(f\"patient_ref: {patient_ref}\")\n",
    "\n",
    "            # Collect encounters\n",
    "            context = resource.get(\"context\", {})\n",
    "            encounter_refs = safe_get(resource, \"context\", \"encounter\") or []\n",
    "            encounters_info = [\n",
    "                {\n",
    "                    \"encounter_ref\": enc.get(\"reference\"),\n",
    "                    \"encounter_display\": enc.get(\"display\")\n",
    "                }\n",
    "                for enc in encounter_refs\n",
    "            ]\n",
    "\n",
    "            date_time = resource.get(\"date\")\n",
    "            #logging.info(f\"date_time: {date_time}\")\n",
    "\n",
    "            # Collect codings\n",
    "            codings = resource.get(\"type\", {}).get(\"coding\", [])\n",
    "            code_info = [\n",
    "                {\n",
    "                    \"code\": c.get(\"code\"),\n",
    "                    \"system\": c.get(\"system\"),\n",
    "                    \"description\": c.get(\"display\")\n",
    "                }\n",
    "                for c in codings\n",
    "            ]\n",
    "\n",
    "            # Collect authors\n",
    "            authors = resource.get(\"author\", [])\n",
    "            author_info = [\n",
    "                {\n",
    "                    \"author_ref\": a.get(\"reference\"),\n",
    "                    \"author_display\": a.get(\"display\")\n",
    "                }\n",
    "                for a in authors\n",
    "            ]\n",
    "\n",
    "            # Collect custodian\n",
    "            custodian = resource.get(\"custodian\", {})\n",
    "            custodian_info = [{\n",
    "                \"custodian_reference\": custodian.get(\"reference\"),\n",
    "                \"custodian_display\": custodian.get(\"display\")\n",
    "            }]\n",
    "\n",
    "            # Document period\n",
    "            period = context.get(\"period\", {})\n",
    "            start_date_time = period.get(\"start\")\n",
    "            end_date_time = period.get(\"end\")\n",
    "            #logging.info(f\"start_date_time: {start_date_time}\")\n",
    "            #logging.info(f\"end_date_time: {end_date_time}\")\n",
    "\n",
    "            # Append to results\n",
    "            document_references.append({\n",
    "                \"document_id\": document_id,\n",
    "                \"status\": status,\n",
    "                \"patient_ref\": patient_ref,\n",
    "                \"encounter_ref\": json.dumps(encounters_info),\n",
    "                \"date_time\": date_time,\n",
    "                \"code_info\": json.dumps(code_info),\n",
    "                \"author_info\": json.dumps(author_info),\n",
    "                \"custodian\": json.dumps(custodian_info),\n",
    "                \"start_date_time\": start_date_time,\n",
    "                \"end_date_time\": end_date_time\n",
    "            })\n",
    "\n",
    "        return document_references\n",
    "\n",
    "\n",
    "def parse_care_plans(file_path):\n",
    "    logger.debug(\"Starting Care Plans parsing\")\n",
    "    logging.info(\"------------------------Parse Care Plans--------------------------------\")\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if \"entry\" not in data:\n",
    "        return []\n",
    "\n",
    "    care_plans = []\n",
    "    for entry in data[\"entry\"]:\n",
    "        resource = entry.get(\"resource\", {})\n",
    "        if resource.get(\"resourceType\") != \"CarePlan\":\n",
    "            continue\n",
    "\n",
    "        ##coding = resource.get(\"coding\")\n",
    "        #logging.info(f\"Care_Plan coding: {coding}\")\n",
    "        careteam_info = []\n",
    "        for ct in resource.get(\"careTeam\", []):\n",
    "            ref = ct.get(\"reference\")\n",
    "            if ref:\n",
    "                ref_id = ref.replace(\"urn:uuid:\", \"\")\n",
    "                careteam_resource = next(\n",
    "                    (e.get(\"resource\") for e in data.get(\"entry\", [])\n",
    "                     if e.get(\"resource\", {}).get(\"id\") == ref_id),\n",
    "                    None\n",
    "                )\n",
    "                if careteam_resource:\n",
    "                    careteam_info.append({\n",
    "                        \"careTeam_id\": ref_id,\n",
    "                        \"name\": careteam_resource.get(\"name\"),\n",
    "                        \"managingOrganization\": careteam_resource.get(\"managingOrganization\"),\n",
    "                        \"participants\": careteam_resource.get(\"participant\", [])\n",
    "                    })\n",
    "\n",
    "        # Resolve addresses references (conditions/problems)\n",
    "        addresses_info = []\n",
    "        for addr in resource.get(\"addresses\", []):\n",
    "            ref = addr.get(\"reference\")\n",
    "            if ref:\n",
    "                ref_id = ref.replace(\"urn:uuid:\", \"\")\n",
    "                condition_resource = next(\n",
    "                    (e.get(\"resource\") for e in data.get(\"entry\", [])\n",
    "                     if e.get(\"resource\", {}).get(\"id\") == ref_id),\n",
    "                    None\n",
    "                )\n",
    "                if condition_resource:\n",
    "                    addresses_info.append({\n",
    "                        \"condition_id\": ref_id,\n",
    "                        \"code\": condition_resource.get(\"code\"),\n",
    "                        \"status\": condition_resource.get(\"clinicalStatus\"),\n",
    "                        \"verificationStatus\": condition_resource.get(\"verificationStatus\")\n",
    "                    })\n",
    "\n",
    "        logging.info(f\"care_plan_id: {resource.get(\"id\")}\")\n",
    "        #logging.info(f\"code: {coding.get(\"code\",[]).get(\"code\")}\")\n",
    "        #logging.info(f\"system: {coding.get(\"system\")}\")\n",
    "        #logging.info(f\"description: {coding.get(\"display\")}\")\n",
    "        coding_info = []\n",
    "        #category = resource.get(\"category\", [])\n",
    "        #logging.info(f\"category: {category}\")\n",
    "        #if category is None:\n",
    "        #    return coding_info\n",
    "\n",
    "        category = resource.get(\"category\", [])\n",
    "\n",
    "        # Normalize so it's always a list\n",
    "        category_data = resource.get(\"category\", [])\n",
    "\n",
    "        # normalize to always be a list\n",
    "        if isinstance(category_data, dict):\n",
    "            category_data = [category_data]\n",
    "\n",
    "        for cat in category_data:\n",
    "            for coding in cat.get(\"coding\", []):\n",
    "                system = coding.get(\"system\")\n",
    "                logging.info(f\"care_plan category-System:  {system}\")\n",
    "                code = coding.get(\"code\")\n",
    "                logging.info(f\"care_plan category- Code: {code}\" )\n",
    "                category = coding.get(\"display\")\n",
    "                logging.info(f\"care_plan category- Display: {category}\")\n",
    "                #for cat in category:\n",
    "                    #for coding in cat.get(\"coding\", []):\n",
    "                    #coding_info.append({\n",
    "                if system:\n",
    "                    coding_info.append(display)    \n",
    "                if code: \n",
    "                    coding_info.append(code)\n",
    "                if display: \n",
    "                     coding_info.append(display)\n",
    "                #})                   \n",
    "                            \n",
    "            #else:\n",
    "                #return coding_info\n",
    "        logging.info(f\"status: {resource.get(\"status\")}\")\n",
    "        logging.info(f\"intent: {resource.get(\"intent\")}\")\n",
    "        logging.info(f\"subject: {safe_get(resource, \"subject\", \"reference\")}\")\n",
    "        logging.info(f\"encounter: {safe_get(resource, \"encounter\", \"reference\")}\")\n",
    "        logging.info(f\"period_start: {safe_get(resource, \"period\", \"start\")}\")\n",
    "        logging.info(f\"care team: {careteam_info}\")\n",
    "        logging.info(f\"addresses: {addresses_info}\")\n",
    "\n",
    "        care_plans.append({\n",
    "            \"care_plan_id\": resource.get(\"id\"),\n",
    "            \"code\": json.dumps(coding_info, default=str),\n",
    "            \"status\": resource.get(\"status\"),\n",
    "            \"intent\": resource.get(\"intent\"),\n",
    "            \"subject_ref\": safe_get(resource, \"subject\", \"reference\"),\n",
    "            \"encounter_ref\": safe_get(resource, \"encounter\", \"reference\"),\n",
    "            \"period_start\": safe_get(resource, \"period\", \"start\"),\n",
    "            \"careteam\": json.dumps(careteam_info),\n",
    "            \"addresses\": json.dumps(addresses_info)\n",
    "        })\n",
    "    return care_plans\n",
    "\n",
    "def deduplicate_supporting_info(info_list):\n",
    "    \"\"\"Remove duplicates based on sequence + reference.\"\"\"\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for info in info_list:\n",
    "        key = (info.get(\"sequence\"), info.get(\"reference\"))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            deduped.append(info)\n",
    "    return deduped\n",
    "\n",
    "def insert_supporting_info(claim_id, info_list):\n",
    "    \"\"\"Insert deduplicated supportingInfo into supporting_info table.\"\"\"\n",
    "    deduped_info = deduplicate_supporting_info(info_list)\n",
    "    rows = [\n",
    "        (   \n",
    "            claim_id, \n",
    "            info.get(\"sequence\"), \n",
    "            info.get(\"reference\"), \n",
    "            info.get(\"display\"), \n",
    "            info.get(\"type\"))\n",
    "        for info in deduped_info\n",
    "    ]\n",
    "\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO supporting_info (claim_id, sequence, reference, display, type)\n",
    "    VALUES %s\n",
    "    ON CONFLICT (claim_id, sequence, reference) DO NOTHING\n",
    "    \"\"\"\n",
    "    #execute_values(cur, insert_query, rows)\n",
    "\n",
    "\"\"\"def parse_claims(file_path):\n",
    "    claims_logger = get_parser_logger(\"claims_parser\", \"C:\\\\Users\\\\tonim\\\\Downloads\\\\output\\\\fhir\\\\claims_parsing_log\")\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        if \"entry\" not in data:\n",
    "                return None\n",
    "        \n",
    "        claims=[]\n",
    "        for entry in data[\"entry\"]:\n",
    "            resource = entry.get(\"resource\", {})\n",
    "            if resource.get(\"resourceType\") == \"Claim\":\n",
    "                claim_id = resource.get(\"id\") \n",
    "                claim_type = resource.get(\"type\",{})\n",
    "                claim_type_codings = claim_type.get(\"coding\",[])\n",
    "                claim_type_info = []\n",
    "                claim__code = None\n",
    "                for coding in claim_type_codings:\n",
    "                    claim_code = coding.get(\"code\")\n",
    "                    claim_system = coding.get(\"system\")\n",
    "                    claims_logger.info(f\"claim_type: {claim_code}, claim_system: {claim_system}\")\n",
    "                    claim_type_info.append({\n",
    "                        \"claim_type\": claim_code,\n",
    "                        \"claim_system\": claim_system\n",
    "                        })\n",
    "                    \n",
    "                claim_status = resource.get(\"status\")\n",
    "                claim_patient = resource.get(\"patient\",{})\n",
    "                claim_patient_ref = claim_patient.get(\"reference\")\n",
    "                claim_total = resource.get(\"total\")\n",
    "                claim_total_value = claim_total.get(\"value\") if claim_total else None\n",
    "                claim_total_currency = claim_total.get(\"currency\") if claim_total else None\n",
    "                billable_period = resource.get(\"billablePeriod\")\n",
    "                billable_start = billable_period.get(\"start\")\n",
    "                billable_end = billable_period.get(\"end\")\n",
    "                bill_created = resource.get(\"created\")\n",
    "                provider = resource.get(\"provider\")\n",
    "                provider_ref = provider.get(\"reference\")\n",
    "                provider_name = provider.get(\"display\")\n",
    "                facility = resource.get(\"facility\") or {}\n",
    "                facility_ref = facility.get(\"reference\")\n",
    "                facility_name = facility.get(\"display\")\n",
    "                priority = resource.get(\"priority\")\n",
    "                coding_data = priority.get(\"coding\",[])\n",
    "                coding_info = []\n",
    "                #encounter_ref = \n",
    "                #logging.info(f\"priority:  {priority}\")\n",
    "\n",
    "                for coding in coding_data:\n",
    "                    code = coding.get(\"code\")\n",
    "                    logging.info(f\"code: {code}\")\n",
    "                    system = coding.get(\"system\")\n",
    "                    logging.info(f\"system: {system}\")\n",
    "                    coding_info.append({\n",
    "                        \"code\": code,\n",
    "                        \"system\": system\n",
    "                                        })\n",
    "                \n",
    "                \n",
    "\n",
    "                claims_logger.info(f\"claim_patient_ref: {claim_patient_ref}\")\n",
    "\n",
    "                claims_logger.info(f\"claim_id: {claim_id}\") \n",
    "                claims_logger.info(f\"claim_status: {claim_status}\")\n",
    "\n",
    "                claims_logger.info(f\"claim_total: {claim_total}\")\n",
    "                claims_logger.info(f\"claim_total_value: {claim_total_value}\")\n",
    "                claims_logger.info(f\"claim_total_currency: {claim_total_currency}\")\n",
    "                claims_logger.info(f\"start: {billable_start}\")\n",
    "                claims_logger.info(f\"end: {billable_end}\")\n",
    "                claims_logger.info(f\"bill_created: {bill_created}\")\n",
    "                claims_logger.info(f\"provider_ref: {provider_ref}\")\n",
    "                claims_logger.info(f\"provider_name: {provider_name}\")\n",
    "\n",
    "                 # Diagnosis---need to test, these are usually observations, or \n",
    "                diagnosis = extract_diagnoses_from_claim(resource)\n",
    "                claims_logger.info(f\"extracted diagnosis: {diagnosis}\")\n",
    "                insurance = extract_insurance_from_claim(resource)\n",
    "                claims_logger.info(f\"extracted insurance: {insurance}\")\n",
    "                #for diag in diagnosis:\n",
    "                #    sequence = diag.get(\"diagnosis\",[])\n",
    "\n",
    "                claims.append({\n",
    "                    \"claim_id\": claim_id,\n",
    "                    \"claim_status\": claim_status,\n",
    "                    \"claim_patient_ref\": claim_patient_ref,\n",
    "                    \"claim_type\":{json.dumps(claim_type_info)},\n",
    "                    \"claim_total\" : f\"{claim_total_value} {claim_total_currency}\",\n",
    "                    \"billable_period\": f\"{billable_start}-{billable_end}\",\n",
    "                    \"bill_date\": bill_created,\n",
    "                    \"provider\": provider_ref,\n",
    "                    \"facility\": facility_ref,\n",
    "                    \"priority\": priority,\n",
    "                    \"coding_info\": coding_info,\n",
    "                    \"claim_diagnosis\": {json.dumps(diagnosis)},\n",
    "                    \"claim_insurance\": {json.dumps(insurance)}\n",
    "                })\n",
    "\n",
    "        return claims\"\"\"\n",
    "def parse_claims(file_path):\n",
    "    claims_logger = get_parser_logger(\n",
    "        \"claims_parser\", \n",
    "        \"C:\\\\Users\\\\tonim\\\\Downloads\\\\output\\\\fhir\\\\claims_parsing_log\"\n",
    "    )\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        if \"entry\" not in data:\n",
    "            return None\n",
    "        \n",
    "        claims_list = []\n",
    "\n",
    "        for entry in data[\"entry\"]:\n",
    "            resource = entry.get(\"resource\", {})\n",
    "            if resource.get(\"resourceType\") != \"Claim\":\n",
    "                continue\n",
    "\n",
    "            claim_id = resource.get(\"id\")\n",
    "            claim_status = resource.get(\"status\")\n",
    "            \n",
    "            # Patient\n",
    "            claim_patient_ref = resource.get(\"patient\", {}).get(\"reference\")\n",
    "            \n",
    "            # Type\n",
    "            claim_type_info = []\n",
    "            claim_type = resource.get(\"type\", {})\n",
    "            for coding in claim_type.get(\"coding\", []):\n",
    "                claim_type_info.append({\n",
    "                    \"code\": coding.get(\"code\"),\n",
    "                    \"system\": coding.get(\"system\")\n",
    "                })\n",
    "\n",
    "            # Total\n",
    "            claim_total = resource.get(\"total\", {})\n",
    "            \n",
    "            # Billable period\n",
    "            billable_period = resource.get(\"billablePeriod\", {})\n",
    "            billable_start = billable_period.get(\"start\")\n",
    "            billable_end = billable_period.get(\"end\")\n",
    "            \n",
    "            # Provider & Facility\n",
    "            provider = resource.get(\"provider\", {})\n",
    "            provider_ref = provider.get(\"reference\")\n",
    "            provider_name = provider.get(\"display\")\n",
    "\n",
    "            facility = resource.get(\"facility\", {})\n",
    "            facility_ref = facility.get(\"reference\")\n",
    "            facility_name = facility.get(\"display\")\n",
    "            \n",
    "            # Priority\n",
    "            priority_coding = []\n",
    "            priority = resource.get(\"priority\", {})\n",
    "            for coding in priority.get(\"coding\", []):\n",
    "                priority_coding.append({\n",
    "                    \"code\": coding.get(\"code\"),\n",
    "                    \"system\": coding.get(\"system\")\n",
    "                })\n",
    "            \n",
    "            # Diagnosis\n",
    "            diagnosis = extract_diagnoses_from_claim(resource)\n",
    "\n",
    "            # Insurance\n",
    "            insurance = extract_insurance_from_claim(resource)\n",
    "\n",
    "            # Items\n",
    "            items = []\n",
    "            for item in resource.get(\"item\", []):\n",
    "                items.append({\n",
    "                    \"sequence\": item.get(\"sequence\"),\n",
    "                    \"productOrService\": item.get(\"productOrService\", {}).get(\"coding\", [{}])[0].get(\"code\"),\n",
    "                    \"productOrServiceText\": item.get(\"productOrService\", {}).get(\"text\"),\n",
    "                    \"encounters\": [enc.get(\"reference\") for enc in item.get(\"encounter\", [])]\n",
    "                })\n",
    "\n",
    "            # Logging\n",
    "            claims_logger.info(f\"Parsing claim {claim_id} for patient {claim_patient_ref}\")\n",
    "\n",
    "            # Construct claim dict\n",
    "            claims_list.append({\n",
    "                \"claim_id\": claim_id,\n",
    "                \"status\": claim_status,\n",
    "                \"patient_ref\": claim_patient_ref,\n",
    "                \"type\": claim_type_info,\n",
    "                \"total\": claim_total,\n",
    "                \"billable_period\": {\"start\": billable_start, \"end\": billable_end},\n",
    "                \"created\": resource.get(\"created\"),\n",
    "                \"provider\": {\"ref\": provider_ref, \"name\": provider_name},\n",
    "                \"facility\": {\"ref\": facility_ref, \"name\": facility_name},\n",
    "                \"priority\": priority_coding,\n",
    "                \"diagnosis\": diagnosis,\n",
    "                \"insurance\": insurance,\n",
    "                \"items\": items,\n",
    "                \"resource\": resource\n",
    "            })\n",
    "\n",
    "        return claims_list\n",
    "\n",
    "\n",
    "\"\"\"def parse_EOB(file_path):\n",
    "    EOB_logger = get_parser_logger(\"EOB_parser\", \"C:\\\\Users\\\\tonim\\\\Downloads\\\\output\\\\fhir\\\\EOB_parsing_log\")\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        if \"entry\" not in data:\n",
    "                return None\n",
    "        \n",
    "        eob=[]\n",
    "        for entry in data[\"entry\"]:\n",
    "            resource = entry.get(\"resource\", {})\n",
    "            if resource.get(\"resourceType\") == \"ExplanationOfBenefit\":\n",
    "                eob_id = resource.get(\"id\") \n",
    "                EOB_extracted_info= []\n",
    "                #print(\"eob_id: \", eob_id)\n",
    "                EOB_info = extract_EOB_info(resource) \n",
    "                for contained in EOB_info:\n",
    "                    performers_raw = contained.get(\"performer\", [])\n",
    "                    if isinstance(performers_raw, dict):\n",
    "                        performers = [performers_raw.get(\"performer\")]\n",
    "                    elif isinstance(performers_raw, list):\n",
    "                        performers = [p.get(\"performer\") for p in performers_raw if isinstance(p,dict)]\n",
    "                    else:\n",
    "                        performers = []\n",
    "\n",
    "                    type = contained.get(\"resourceType\")\n",
    "                    id = contained.get(\"id\")\n",
    "                    status = contained.get(\"status\")\n",
    "                    intent = contained.get(\"intent\")\n",
    "                    subject = contained.get(\"subject\", {}).get(\"reference\")\n",
    "                    requester = contained.get(\"requester\", {}).get(\"reference\")\n",
    "                    EOB_logger.debug(\"type: {type}\")\n",
    "                    EOB_logger.debug(\"id: {id}\")\n",
    "                    EOB_logger.debug(\"status: {status}\")\n",
    "                    \n",
    "                    EOB_extracted_info.append({\n",
    "                        \"type\": type,\n",
    "                        \"id\": id,\n",
    "                        \"status\": status,\n",
    "                        \"intent\": intent,\n",
    "                        \"subject\": subject,\n",
    "                        \"requester\": requester,\n",
    "                        \"performer\": performers\n",
    "                    })\n",
    "\n",
    "                EOB_logger.debug(f\"EOB info: {EOB_info}\")\n",
    "\n",
    "                eob.append({\n",
    "                    \"eob_id\": eob_id,\n",
    "                    \"extracted_info\": {json.dumps(EOB_extracted_info)}\n",
    "                })\n",
    "\n",
    "        return eob\"\"\"  \n",
    "\n",
    "def parse_EOB(file_path):\n",
    "    EOB_logger = get_parser_logger(\n",
    "        \"EOB_parser\", \n",
    "        \"C:\\\\Users\\\\tonim\\\\Downloads\\\\output\\\\fhir\\\\EOB_parsing_log\"\n",
    "    )\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        if \"entry\" not in data:\n",
    "            return None\n",
    "        \n",
    "        eob_list = []\n",
    "\n",
    "        for entry in data[\"entry\"]:\n",
    "            resource = entry.get(\"resource\", {})\n",
    "            if resource.get(\"resourceType\") != \"ExplanationOfBenefit\":\n",
    "                continue\n",
    "\n",
    "            # Top-level fields\n",
    "            eob_id = resource.get(\"id\")\n",
    "            patient_ref = resource.get(\"patient\", {}).get(\"reference\")\n",
    "            status = resource.get(\"status\")\n",
    "            eob_type = resource.get(\"type\")\n",
    "            use = resource.get(\"use\")\n",
    "            created = resource.get(\"created\")\n",
    "\n",
    "            # Insurance references (capture all insurers)\n",
    "            insurance_list = resource.get(\"insurance\", [])\n",
    "            insurer_refs = [\n",
    "                ins.get(\"provider\", {}).get(\"reference\") \n",
    "                for ins in insurance_list\n",
    "                if ins.get(\"provider\")\n",
    "            ]\n",
    "\n",
    "            # Supporting info / performers\n",
    "            extracted_info = []\n",
    "            supporting_info = resource.get(\"supportingInfo\", [])\n",
    "            for info in supporting_info:\n",
    "                performers_raw = info.get(\"performer\", [])\n",
    "                if isinstance(performers_raw, dict):\n",
    "                    performers = [performers_raw.get(\"actor\", {}).get(\"reference\")]\n",
    "                elif isinstance(performers_raw, list):\n",
    "                    performers = [\n",
    "                        p.get(\"actor\", {}).get(\"reference\") \n",
    "                        for p in performers_raw if isinstance(p, dict)\n",
    "                    ]\n",
    "                else:\n",
    "                    performers = []\n",
    "\n",
    "                extracted_info.append({\n",
    "                    \"resourceType\": info.get(\"resourceType\"),\n",
    "                    \"id\": info.get(\"id\"),\n",
    "                    \"status\": info.get(\"status\"),\n",
    "                    \"intent\": info.get(\"intent\"),\n",
    "                    \"subject\": info.get(\"subject\", {}).get(\"reference\"),\n",
    "                    \"requester\": info.get(\"requester\", {}).get(\"reference\"),\n",
    "                    \"performers\": performers\n",
    "                })\n",
    "\n",
    "            EOB_logger.debug(\n",
    "                f\"EOB id {eob_id}, patient {patient_ref}, insurers {insurer_refs}, \"\n",
    "                f\"supporting info: {extracted_info}\"\n",
    "            )\n",
    "\n",
    "            eob_list.append({\n",
    "                \"eob_id\": eob_id,\n",
    "                \"patient_id\": patient_ref,\n",
    "                \"status\": status,\n",
    "                \"type\": eob_type,\n",
    "                \"use\": use,\n",
    "                \"created\": created,\n",
    "                \"insurer_refs\": insurer_refs,\n",
    "                \"extracted_info\": extracted_info,\n",
    "                \"resource\": resource\n",
    "            })\n",
    "\n",
    "        return eob_list\n",
    "\n",
    "\"\"\"def parse_immunizations(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        if \"entry\" not in data:\n",
    "                return None\n",
    "        \n",
    "        immunizations=[]\n",
    "        for entry in data[\"entry\"]:\n",
    "            resource = entry.get(\"resource\", {})\n",
    "            if resource.get(\"resourceType\") == \"Immunization\":\n",
    "                immunization_id = resource.get(\"id\") \n",
    "                status = resource.get(\"status\")\n",
    "                patient_ref = resource.get(\"patient\",{}).get(\"reference\")\n",
    "                encounter_ref = resource.get(\"encounter\",{}).get(\"reference\")\n",
    "                occurrence_date_time = resource.get(\"occurrenceDateTime\")\n",
    "                primary_source = resource.get(\"primarySource\")\n",
    "                location_ref = resource.get(\"location\", {}).get(\"reference\")\n",
    "                location_display = resource.get(\"location\", {}).get(\"display\")\n",
    "\n",
    "                logger.debug(f\"immunization_id: {immunization_id}\")\n",
    "\n",
    "                immunizations.append({\n",
    "                    \"immunization_id\": immunization_id\n",
    "                })\n",
    "\n",
    "        return immunizations \"\"\"\n",
    "\n",
    "def parse_immunizations(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        if \"entry\" not in data:\n",
    "            return None\n",
    "        \n",
    "        immunizations = []\n",
    "\n",
    "        for entry in data[\"entry\"]:\n",
    "            resource = entry.get(\"resource\", {})\n",
    "            if resource.get(\"resourceType\") != \"Immunization\":\n",
    "                continue\n",
    "\n",
    "            immunization_id = resource.get(\"id\")\n",
    "            status = resource.get(\"status\")\n",
    "            patient_ref = resource.get(\"patient\", {}).get(\"reference\")\n",
    "            encounter_ref = resource.get(\"encounter\", {}).get(\"reference\")\n",
    "            occurrence_date_time = resource.get(\"occurrenceDateTime\")\n",
    "            primary_source = resource.get(\"primarySource\")\n",
    "            \n",
    "            location = resource.get(\"location\", {})\n",
    "            location_ref = location.get(\"reference\")\n",
    "            location_display = location.get(\"display\")\n",
    "\n",
    "            # Vaccine code\n",
    "            vaccine_code = resource.get(\"vaccineCode\", {})\n",
    "            vaccine_codings = vaccine_code.get(\"coding\", [])\n",
    "            '''vaccine_info = [\n",
    "                {\"system\": c.get(\"system\"), \"code\": c.get(\"code\"), \"display\": c.get(\"display\")}\n",
    "                for c in vaccine_codings\n",
    "            ]'''\n",
    "            vaccine_info = list(map(lambda c: {\"system\": c.get(\"system\"), \"code\": c.get(\"code\"), \"display\": c.get(\"display\")}, vaccine_codings))\n",
    "\n",
    "\n",
    "            # Performer(s)\n",
    "            performers_raw = resource.get(\"performer\", [])\n",
    "            performers = []\n",
    "            for p in performers_raw:\n",
    "                actor = p.get(\"actor\", {})\n",
    "                performers.append({\n",
    "                    \"actor_ref\": actor.get(\"reference\"),\n",
    "                    \"actor_display\": actor.get(\"display\"),\n",
    "                    \"function\": p.get(\"function\", {}).get(\"text\")\n",
    "                })\n",
    "\n",
    "            immunizations.append({\n",
    "                \"immunization_id\": immunization_id,\n",
    "                \"status\": status,\n",
    "                \"patient_ref\": patient_ref,\n",
    "                \"encounter_ref\": encounter_ref,\n",
    "                \"occurrence_date_time\": occurrence_date_time,\n",
    "                \"primary_source\": primary_source,\n",
    "                \"location\": {\"ref\": location_ref, \"display\": location_display},\n",
    "                \"vaccine_info\": vaccine_info,\n",
    "                \"performers\": performers,\n",
    "                \"resource\": resource\n",
    "            })\n",
    "\n",
    "    return immunizations\n",
    "\n",
    "\n",
    "\"\"\"def parse_coverage(file_path):\n",
    "    #print(\"inside coverage\")\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        if \"entry\" not in data:\n",
    "            return None\n",
    "        \n",
    "        coverage = []\n",
    "        for entry in data[\"entry\"]:\n",
    "            resource = entry.get(\"resource\", {})\n",
    "\n",
    "            # Case 1: Coverage is a top-level resource\n",
    "            if resource.get(\"resourceType\") == \"Coverage\":\n",
    "                #print(\"Found top-level Coverage\")\n",
    "                #print(\"coverage_id\", resource.get(\"id\"))\n",
    "                \n",
    "                coverage.append({\n",
    "                    \"coverage_id\": resource.get(\"id\"),\n",
    "                    \"status\": resource.get(\"status\"),\n",
    "                    \"type\": resource.get(\"type\", {}).get(\"text\"),\n",
    "                })\n",
    "\n",
    "            # Case 2: Coverage inside \"contained\" resources (like in EOB)\n",
    "            #for contained in resource.get(\"contained\", []):\n",
    "                #if contained.get(\"resourceType\") == \"Coverage\":\n",
    "                    #print(\"Found contained Coverage\")\n",
    "                    #print(\"coverage_id: \",  contained.get(\"id\"))\n",
    "                    #coverage.append({\n",
    "                    #    \"coverage_id\": contained.get(\"id\"),\n",
    "                    #    \"status\": contained.get(\"status\"),\n",
    "                    #    \"type\": contained.get(\"type\", {}).get(\"text\"),\n",
    "                    #})\n",
    "\n",
    "        return coverage\"\"\"\n",
    "\n",
    "def parse_coverage(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        if \"entry\" not in data:\n",
    "            return None\n",
    "        \n",
    "        coverage_list = []\n",
    "\n",
    "        for entry in data[\"entry\"]:\n",
    "            resource = entry.get(\"resource\", {})\n",
    "\n",
    "            if resource.get(\"resourceType\") == \"Coverage\":\n",
    "                coverage_list.append({\n",
    "                    \"coverage_id\": resource.get(\"id\"),\n",
    "                    \"status\": resource.get(\"status\"),\n",
    "                    \"type\": resource.get(\"type\", {}),  # keep full dict for JSONB\n",
    "                    \"subscriber_id\": resource.get(\"subscriberId\"),\n",
    "                    \"beneficiary\": resource.get(\"beneficiary\", {}).get(\"reference\"),\n",
    "                    \"payor\": [p.get(\"reference\") for p in resource.get(\"payor\", [])],\n",
    "                    \"period\": resource.get(\"period\"),\n",
    "                    \"class\": resource.get(\"class\", []),\n",
    "                    \"resource\": resource  # store full raw FHIR JSON\n",
    "                })\n",
    "\n",
    "            # Check contained resources too\n",
    "            for contained in resource.get(\"contained\", []):\n",
    "                if contained.get(\"resourceType\") == \"Coverage\":\n",
    "                    coverage_list.append({\n",
    "                        \"coverage_id\": contained.get(\"id\"),\n",
    "                        \"status\": contained.get(\"status\"),\n",
    "                        \"type\": contained.get(\"type\", {}),\n",
    "                        \"subscriber_id\": contained.get(\"subscriberId\"),\n",
    "                        \"beneficiary\": contained.get(\"beneficiary\", {}).get(\"reference\"),\n",
    "                        \"payor\": [p.get(\"reference\") for p in contained.get(\"payor\", [])],\n",
    "                        \"period\": contained.get(\"period\"),\n",
    "                        \"class\": contained.get(\"class\", []),\n",
    "                        \"resource\": contained\n",
    "                    })\n",
    "\n",
    "        return coverage_list\n",
    "\n",
    "\n",
    "def parse_devices(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        if \"entry\" not in data:\n",
    "            return None\n",
    "        \n",
    "        devices = []\n",
    "        for entry in data[\"entry\"]:\n",
    "            resource = entry.get(\"resource\", {})\n",
    "            if resource.get(\"resourceType\") == \"Device\":\n",
    "                device_id = resource.get(\"id\") \n",
    "                status = resource.get(\"status\")\n",
    "                distinct_identifier = resource.get(\"distinctIdentifier\")\n",
    "                manufacture_date = resource.get(\"manufactureDate\")\n",
    "                expiration_date = resource.get(\"expirationDate\")\n",
    "                lot_number = resource.get(\"lotNumber\")\n",
    "                serial_number = resource.get(\"serialNumber\")\n",
    "                patient_ref = resource.get(\"patient\", {}).get(\"reference\")\n",
    "\n",
    "                # collect device names\n",
    "                device_names = [\n",
    "                    {\n",
    "                        \"name\": dn.get(\"name\"),\n",
    "                        \"type\": dn.get(\"type\")\n",
    "                    }\n",
    "                    for dn in resource.get(\"deviceName\", [])\n",
    "                ]\n",
    "\n",
    "                # collect codings\n",
    "                codings = [\n",
    "                    {\n",
    "                        \"system\": c.get(\"system\"),\n",
    "                        \"code\": c.get(\"code\"),\n",
    "                        \"display\": c.get(\"display\")\n",
    "                    }\n",
    "                    for c in resource.get(\"type\", {}).get(\"coding\", [])\n",
    "                ]\n",
    "                \n",
    "                logging.info(f\"device_id: {device_id}\")\n",
    "                devices.append({\n",
    "                    \"device_id\": device_id,\n",
    "                    \"status\": status,\n",
    "                    \"distinct_identifier\": distinct_identifier,\n",
    "                    \"manufacture_date\": manufacture_date,\n",
    "                    \"expiration_date\": expiration_date,\n",
    "                    \"lot_number\": lot_number,\n",
    "                    \"serial_number\": serial_number,\n",
    "                    \"patient_ref\": patient_ref,\n",
    "                    \"device_names\": device_names,\n",
    "                    \"codings\": codings,\n",
    "                    \"resource\": resource  # optional: keep raw JSON for completeness\n",
    "                })   \n",
    "        return devices\n",
    "\n",
    "def parse_medication_request(resource):\n",
    "    return {\n",
    "        \"medrequest_id\": resource.get(\"id\"),\n",
    "        \"status\": resource.get(\"status\"),\n",
    "        \"intent\": resource.get(\"intent\"),\n",
    "        \"medication_id\": resource.get(\"medicationReference\", {}).get(\"reference\"),\n",
    "        \"subject_patient_id\": resource.get(\"subject\", {}).get(\"reference\"),\n",
    "        \"encounter_id\": resource.get(\"encounter\", {}).get(\"reference\"),\n",
    "        \"requester_practitioner_id\": resource.get(\"requester\", {}).get(\"reference\"),\n",
    "        \"requester_org_id\": resource.get(\"requester\", {}).get(\"organization\", {}).get(\"reference\"),\n",
    "        \"authored_on\": resource.get(\"authoredOn\"),\n",
    "        \"dosage_instruction\": resource.get(\"dosageInstruction\"),\n",
    "        \"dispense_request\": resource.get(\"dispenseRequest\"),\n",
    "        \"resource\": resource\n",
    "    }\n",
    "\n",
    "def parse_supply_delivery(resource):\n",
    "    return {\n",
    "        \"supplydelivery_id\": resource.get(\"id\"),\n",
    "        \"status\": resource.get(\"status\"),\n",
    "        \"type\": resource.get(\"type\"),\n",
    "        \"patient_id\": resource.get(\"patient\", {}).get(\"reference\"),\n",
    "        \"supplier_org_id\": resource.get(\"supplier\", {}).get(\"reference\"),\n",
    "        \"destination_org_id\": resource.get(\"destination\", {}).get(\"organization\", {}).get(\"reference\"),\n",
    "        \"occurrence\": resource.get(\"occurrenceDateTime\"),\n",
    "        \"supplied_item\": resource.get(\"suppliedItem\"),\n",
    "        \"resource\": resource\n",
    "    }\n",
    "\n",
    "def parse_procedure(resource):\n",
    "    return {\n",
    "        \"procedure_id\": resource.get(\"id\"),\n",
    "        \"status\": resource.get(\"status\"),\n",
    "        \"category\": resource.get(\"category\"),\n",
    "        \"code\": resource.get(\"code\"),\n",
    "        \"subject_patient_id\": resource.get(\"subject\", {}).get(\"reference\"),\n",
    "        \"encounter_id\": resource.get(\"encounter\", {}).get(\"reference\"),\n",
    "        \"performer_practitioner_id\": (resource.get(\"performer\", [{}])[0].get(\"actor\", {}).get(\"reference\")),\n",
    "        \"performer_org_id\": (resource.get(\"performer\", [{}])[0].get(\"onBehalfOf\", {}).get(\"reference\")),\n",
    "        \"reason_code\": resource.get(\"reasonCode\"),\n",
    "        \"reason_reference\": resource.get(\"reasonReference\"),\n",
    "        \"body_site\": resource.get(\"bodySite\"),\n",
    "        \"outcome\": resource.get(\"outcome\"),\n",
    "        \"follow_up\": resource.get(\"followUp\"),\n",
    "        \"complication\": resource.get(\"complication\"),\n",
    "        \"performed_period\": resource.get(\"performedPeriod\"),\n",
    "        \"resource\": resource\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# clean for empty field values\n",
    "def clean_nested(data):\n",
    "    if isinstance(data, dict):\n",
    "        if not data:  # empty dict\n",
    "            return None\n",
    "        return {k: clean_nested(v) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [clean_nested(v) for v in data]\n",
    "    return data\n",
    "\n",
    "# You can follow the same pattern for document_reference, immunizations, claims, EOB, coverage\n",
    "# -------------------------\n",
    "# Test with one file and save to parquet\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create a logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    # File handler\n",
    "    file_handler = logging.FileHandler('C:\\\\Users\\\\tonim\\\\Downloads\\\\output\\\\fhir\\\\debug.log')\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "\n",
    "    # Formatter\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    # Add handlers to logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    # Usage\n",
    "    #logger.debug(\"Debug message goes to file\")\n",
    "    #logger.info(\"Info message goes to both console and file\")\n",
    "\n",
    "\n",
    "\n",
    "    #test_file = r\"C:\\Users\\tonim\\Downloads\\output\\fhir\\Patients\\German382_Runte676_badeac37-4850-29ae-978e-3435d3358c85.json\"\n",
    "    #test_file = r\"C:\\Users\\tonim\\Downloads\\output\\fhir\\Patients\\Herman763_Sanford861_994754ee-a5d2-495c-90f4-fd4ada07bf02.json\"\n",
    "    test_file = r\"C:\\Users\\tonim\\Downloads\\output\\fhir\\Patients\\Floyd420_Streich926_42f4db2f-b049-c9a1-a961-7a944ea72e48.json\"\n",
    "    patients_output_file = r\"C:\\Users\\tonim\\Downloads\\output\\parquet\\patients.parquet\"\n",
    "    encounters_output_file = r\"C:\\Users\\tonim\\Downloads\\output\\parquet\\encounters.parquet\"\n",
    "    conditions_output_file = r\"C:\\Users\\tonim\\Downloads\\output\\parquet\\conditions.parquet\"\n",
    "    observations_output_file = r\"C:\\Users\\tonim\\Downloads\\output\\parquet\\observations.parquet\"\n",
    "    diagnostic_reports_output_file = r\"C:\\Users\\tonim\\Downloads\\output\\parquet\\diagnostic_reports.parquet\"\n",
    "    care_plans_output_file = r\"C:\\Users\\tonim\\Downloads\\output\\parquet\\care_plans_reports.parquet\"\n",
    "    claims_output_file = r\"C:\\Users\\tonim\\Downloads\\output\\parquet\\claims.parquet\"\n",
    "    eob_output_file = r\"C:\\Users\\tonim\\Downloads\\output\\parquet\\eob.parquet\"\n",
    "    coverage_output_file = r\"C:\\Users\\tonim\\Downloads\\output\\parquet\\coverage.parquet\"\n",
    "    document_reference_output_file = r\"C:\\Users\\tonim\\Downloads\\output\\parquet\\document_reference.parquet\"\n",
    "    immunization_output_file = r\"C:\\Users\\tonim\\Downloads\\output\\parquet\\immunization_output.parquet\"\n",
    "    device_output_file = r\"C:\\Users\\tonim\\Downloads\\output\\parquet\\immunization_output.parquet\"\n",
    "\n",
    "    #os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    patients_data = [clean_nested(p) for p in (parse_patient_file(test_file))]\n",
    "    encounter_data = parse_encounter_file(test_file)\n",
    "    condition_data = insert_conditions(test_file)\n",
    "    observation_data = parse_observations(test_file)\n",
    "    diagnostic_data = parse_diagnostic_reports(test_file)\n",
    "    care_plans_data = parse_care_plans(test_file)\n",
    "    claim_data = parse_claims(test_file)\n",
    "    eob_data = parse_EOB(test_file)\n",
    "    coverage_data = parse_coverage(test_file)\n",
    "    document_reference_data = parse_document_reference(test_file)\n",
    "    immunization_data = parse_immunizations(test_file)\n",
    "    device_data = parse_devices(test_file)\n",
    "\n",
    "if patients_data:\n",
    "    df_patients = pd.DataFrame(patients_data)\n",
    "    print(df_patients)\n",
    "    df_patients.to_parquet(patients_output_file, engine=\"pyarrow\", index=False)\n",
    "    print(f\"Wrote {len(df_patients)} patients to {patients_output_file}\")\n",
    "else:\n",
    "    print(\"No patient records found.\")\n",
    "\n",
    "if encounter_data:\n",
    "    df_encounters = pd.DataFrame(encounter_data)\n",
    "    print(df_encounters)\n",
    "    df_encounters.to_parquet(encounters_output_file, engine=\"pyarrow\", index=False)\n",
    "    print(f\"Wrote {len(df_encounters)} encounters to {encounters_output_file}\")\n",
    "else:\n",
    "    print(\"No encounter records found.\")\n",
    "\n",
    "if condition_data:\n",
    "    df_conditions = pd.DataFrame(condition_data)\n",
    "    print(df_conditions)\n",
    "    df_conditions.to_parquet(conditions_output_file, engine=\"pyarrow\", index=False)\n",
    "    print(f\"Wrote {len(df_conditions)} conditions to {conditions_output_file}\")\n",
    "else:\n",
    "    print(\"No condition records found.\")\n",
    "\n",
    "if observation_data:\n",
    "    df_observations = pd.DataFrame(observation_data)\n",
    "    print(df_observations)\n",
    "    df_observations.to_parquet(observations_output_file, engine=\"pyarrow\", index=False)\n",
    "    print(f\"Wrote {len(df_observations)} observations to {observations_output_file}\")\n",
    "else:\n",
    "    print(\"No observations records found.\")\n",
    "\n",
    "if diagnostic_data:\n",
    "    df_diagnostic_reports = pd.DataFrame(diagnostic_data)\n",
    "    print(df_diagnostic_reports)\n",
    "    df_diagnostic_reports.to_parquet(diagnostic_reports_output_file, engine=\"pyarrow\", index=False)\n",
    "    print(f\"Wrote {len(df_diagnostic_reports)} diagnosis to {diagnostic_reports_output_file}\")\n",
    "else:\n",
    "    print(\"No diagnostic report records found.\")\n",
    "\n",
    "if immunization_data:\n",
    "    df_immunizations = pd.DataFrame(immunization_data)\n",
    "    print(df_immunizations)\n",
    "    df_immunizations.to_parquet(immunization_output_file, engine=\"pyarrow\", index=False)\n",
    "    print(f\"Wrote {len(df_immunizations)} diagnosis to {immunization_output_file}\")\n",
    "else:\n",
    "    print(\"No immunization records found.\")\n",
    "\n",
    "if document_reference_data:\n",
    "    df_document_reference = pd.DataFrame(document_reference_data)\n",
    "    print(document_reference_data)\n",
    "    df_document_reference.to_parquet(document_reference_output_file, engine=\"pyarrow\", index=False)\n",
    "    print(f\"Wrote {len(df_document_reference)} diagnosis to {document_reference_output_file}\")\n",
    "else:\n",
    "    print(\"No document_reference records found.\")\n",
    "\n",
    "if care_plans_data:\n",
    "    df_care_plans_reports = pd.DataFrame(care_plans_data)\n",
    "    print(df_care_plans_reports)\n",
    "    df_care_plans_reports.to_parquet(care_plans_output_file, engine=\"pyarrow\", index=False)\n",
    "    print(f\"Wrote {len(df_care_plans_reports)} care_plans to {care_plans_output_file}\")\n",
    "else:\n",
    "    print(\"No care plans records found.\")\n",
    "\n",
    "if claim_data:\n",
    "    df_claims = pd.DataFrame(claim_data)\n",
    "    print(df_claims)\n",
    "    df_claims.to_parquet(claims_output_file, engine=\"pyarrow\", index=False)\n",
    "    print(f\"Wrote {len(df_claims)} claim to {claims_output_file}\")\n",
    "else:\n",
    "    print(\"No claim records found.\")\n",
    "\n",
    "if eob_data:\n",
    "    df_eob = pd.DataFrame(eob_data)\n",
    "    print(df_eob)\n",
    "    df_eob.to_parquet(eob_output_file, engine=\"pyarrow\", index=False)\n",
    "    print(f\"Wrote {len(df_eob)} eob to {eob_output_file}\")\n",
    "else:\n",
    "    print(\"No EOB records found.\")\n",
    "\n",
    "if coverage_data:\n",
    "    df_coverage = pd.DataFrame(coverage_data)\n",
    "    print(df_coverage)\n",
    "    df_coverage.to_parquet(coverage_output_file, engine=\"pyarrow\", index=False)\n",
    "    print(f\"Wrote {len(df_coverage)} coverage to {coverage_output_file}\")\n",
    "else:\n",
    "    print(\"No coverage records found.\")\n",
    "\n",
    "if device_data:\n",
    "    df_device = pd.DataFrame(device_data)\n",
    "    logging.info(\"{df_device}\")\n",
    "    df_device.to_parquet(device_output_file, engine=\"pyarrow\", index=False)\n",
    "    logging.info(f\"Wrote{df_device} device to {device_output_file}\")\n",
    "else:\n",
    "    logging.info(f\"No device records found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28fa636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logger\n",
    "import logging\n",
    "def logger(name, log_file, level=logging.DEBUG):\n",
    "    \"\"\"Create a dedicated logger for a parser method.\"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    \n",
    "    # Avoid adding multiple handlers if logger already exists\n",
    "    if not logger.handlers:\n",
    "        file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')\n",
    "        file_handler.setLevel(level)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360abad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_nested(data):\n",
    "    \"\"\"Recursively replace empty dicts/lists with None.\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        if not data:\n",
    "            return None\n",
    "        return {k: clean_nested(v) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        if not data:\n",
    "            return None\n",
    "        return [clean_nested(v) for v in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf5449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def safe_get(value, key=None, default=None):\n",
    "    \"\"\"Safe extraction from dicts/lists.\"\"\"\n",
    "    if key is None:\n",
    "        return value if value is not None else default\n",
    "    if isinstance(value, dict):\n",
    "        return value.get(key, default)\n",
    "    return default''' \n",
    "\n",
    "''''def safe_get(d, *keys):\n",
    "    for k in keys:\n",
    "        if isinstance(d, dict):\n",
    "            d = d.get(k)\n",
    "        else:\n",
    "            return None\n",
    "    return d''' \n",
    "def get_field(resource, *keys):\n",
    "    \"\"\"Safely navigate nested dictionaries/lists. Returns None if missing.\"\"\"\n",
    "    value = resource\n",
    "    for key in keys:\n",
    "        if isinstance(value, list):\n",
    "            value = value[0] if value else None\n",
    "        if isinstance(value, dict):\n",
    "            value = value.get(key)\n",
    "        else:\n",
    "            return None\n",
    "    return value\n",
    "\n",
    "# --- Helpers ---\n",
    "def clean_nested(data):\n",
    "    \"\"\"Recursively replace empty dicts/lists with None.\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        if not data:\n",
    "            return None\n",
    "        return {k: clean_nested(v) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        if not data:\n",
    "            return None\n",
    "        return [clean_nested(v) for v in data]\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def safe_get(d, *keys, default=None):\n",
    "    \"\"\"Safely extract nested values from a dict.\"\"\"\n",
    "    for k in keys:\n",
    "        if isinstance(d, dict):\n",
    "            d = d.get(k, default)\n",
    "        else:\n",
    "            return default\n",
    "    return d\n",
    "\n",
    "\n",
    "def extract_identifier(resource, code):\n",
    "    \"\"\"Extract an identifier of a given type code (SS, DL, PPN).\"\"\"\n",
    "    for ident in resource.get(\"identifier\", []):\n",
    "        type_code = safe_get(ident.get(\"type\", {}).get(\"coding\", [{}])[0], \"code\")\n",
    "        if type_code == code:\n",
    "            return ident.get(\"value\")\n",
    "    return None\n",
    "\n",
    "def extract_extensions(resource, url):\n",
    "    \"\"\"Extract FHIR extensions matching a URL.\"\"\"\n",
    "    result = {}\n",
    "    for ext in resource.get(\"extension\", []) or []:\n",
    "        if ext.get(\"url\") == url:\n",
    "            for inner in ext.get(\"extension\", []) or []:\n",
    "                key = inner.get(\"url\")\n",
    "                value = inner.get(\"valueString\") or safe_get(inner, \"valueCoding\", \"display\")\n",
    "                if key:\n",
    "                    result[key] = value\n",
    "    return result if result else None\n",
    "\n",
    "def extract_geolocation(address):\n",
    "    \"\"\"Extract latitude/longitude from a FHIR address extension.\"\"\"\n",
    "    latitude = longitude = None\n",
    "    for ext in address.get(\"extension\", []) or []:\n",
    "        if ext.get(\"url\") == \"http://hl7.org/fhir/StructureDefinition/geolocation\":\n",
    "            for geo in ext.get(\"extension\", []) or []:\n",
    "                if geo.get(\"url\") == \"latitude\":\n",
    "                    latitude = geo.get(\"valueDecimal\")\n",
    "                elif geo.get(\"url\") == \"longitude\":\n",
    "                    longitude = geo.get(\"valueDecimal\")\n",
    "    return latitude, longitude\n",
    "\n",
    "def extract_birth_place(resource):\n",
    "    \"\"\"Return formatted birth_place string or None.\"\"\"\n",
    "    extensions = resource.get(\"extension\", []) or []\n",
    "    birth_place = next(\n",
    "        (ext.get(\"valueAddress\") for ext in extensions \n",
    "         if ext.get(\"url\") == \"http://hl7.org/fhir/StructureDefinition/patient-birthPlace\"),\n",
    "        None\n",
    "    )\n",
    "    if birth_place:\n",
    "        parts = [birth_place.get(\"city\"), birth_place.get(\"state\"), birth_place.get(\"country\")]\n",
    "        return \", \".join([p for p in parts if p]) or None\n",
    "    return None\n",
    "\n",
    "def extract_addresses(resource):\n",
    "    \"\"\"Return list of addresses with geolocation if available.\"\"\"\n",
    "    addresses = []\n",
    "    for addr in resource.get(\"address\", []) or []:\n",
    "        lat, lon = extract_geolocation(addr)\n",
    "        addresses.append({\n",
    "            \"line\": addr.get(\"line\"),\n",
    "            \"city\": addr.get(\"city\"),\n",
    "            \"state\": addr.get(\"state\"),\n",
    "            \"postal_code\": addr.get(\"postalCode\"),\n",
    "            \"country\": addr.get(\"country\"),\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon\n",
    "        })\n",
    "    return addresses if addresses else None\n",
    "\n",
    "def extract_telecoms(resource):\n",
    "    \"\"\"Return list of telecoms (phone/email/etc).\"\"\"\n",
    "    telecoms_list = []\n",
    "    for t in resource.get(\"telecom\", []) or []:\n",
    "        telecoms_list.append({\n",
    "            \"system\": t.get(\"system\"),\n",
    "            \"value\": t.get(\"value\"),\n",
    "            \"use\": t.get(\"use\"),\n",
    "            \"extension\": t.get(\"extension\")  # e.g., US Core Direct\n",
    "        })\n",
    "    return telecoms_list if telecoms_list else None\n",
    "\n",
    "def extract_text_or_display(obj, text_key=\"text\", coding_key=\"coding\", category_key=\"ombCategory\"):\n",
    "    \"\"\"\n",
    "    Extracts a human-readable string from a FHIR object like maritalStatus, race, ethnicity.\n",
    "    Priority: text -> coding.display -> ombCategory.display\n",
    "    \"\"\"\n",
    "    if not isinstance(obj, dict):\n",
    "        return None\n",
    "\n",
    "    # Prefer the plain \"text\" field\n",
    "    if text_key in obj and obj[text_key]:\n",
    "        return obj[text_key]\n",
    "\n",
    "    # Fallback to first coding.display\n",
    "    if coding_key in obj and isinstance(obj[coding_key], list) and obj[coding_key]:\n",
    "        return obj[coding_key][0].get(\"display\")\n",
    "\n",
    "    # Fallback to ombCategory.display (if race/ethnicity style)\n",
    "    if category_key in obj and isinstance(obj[category_key], dict):\n",
    "        return obj[category_key].get(\"display\")\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e7bc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "def insert_patient: \n",
    "    \n",
    "\n",
    "# Create a proper logger\n",
    "    Main_logger = logging.getLogger(\"FHIR_ETL\")\n",
    "    Main_logger.setLevel(logging.DEBUG)\n",
    "    if not Main_logger.handlers:\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(logging.DEBUG)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        ch.setFormatter(formatter)\n",
    "        Main_logger.addHandler(ch)\n",
    "        conn = psycopg2.connect(\"dbname=FHIR_staging user=postgres password=new_password host=localhost\")\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Insert each patient\n",
    "        #for patient_data in patients_data:\n",
    "        \"\"\"\n",
    "            Inserts a patient into fhir_staging.patients_fhir_raw.\n",
    "            Handles JSON fields and optional values safely.\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "        columns = [\n",
    "            \"patient_id\",\n",
    "            \"first_name\",\n",
    "            \"last_name\",\n",
    "            \"prefix\",\n",
    "            \"gender\",\n",
    "            \"birth_date\",\n",
    "            \"deceased_date_time\",\n",
    "            \"ssn\",\n",
    "            \"drivers_license\",\n",
    "            \"passport\",\n",
    "            \"marital_status\",\n",
    "            \"race\",\n",
    "            \"ethnicity\",\n",
    "            \"birth_place\",\n",
    "            \"resource\"\n",
    "        ]\n",
    "\n",
    "    # Define which columns are JSONB\n",
    "    jsonb_columns = {\"race\", \"ethnicity\", \"birth_place\", \"resource\"}\n",
    "\n",
    "\n",
    "    values = []\n",
    "    for col in columns:\n",
    "        val = patient_data.get(col)\n",
    "        if col in jsonb_columns:\n",
    "            if isinstance(val, (dict, list)):\n",
    "                val = json.dumps(val)\n",
    "            elif isinstance(val, str):\n",
    "                # Wrap strings as JSON object {\"text\": \"...\"} for JSONB\n",
    "                val = json.dumps({\"text\": val})\n",
    "            else:\n",
    "                val = json.dumps({})\n",
    "        values.append(val)\n",
    "    \n",
    "    # Build the SQL query safely using psycopg2.sql\n",
    "    query = sql.SQL(\"\"\"\n",
    "        INSERT INTO fhir_staging.patients_fhir_raw ({fields})\n",
    "        VALUES ({placeholders})\n",
    "        ON CONFLICT (patient_id) DO NOTHING\n",
    "        \"\"\").format(\n",
    "        fields=sql.SQL(', ').join(map(sql.Identifier, columns)),\n",
    "        placeholders=sql.SQL(', ').join(sql.Placeholder() * len(columns))\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        cur.execute(query, values)\n",
    "        if logger:\n",
    "            logger.debug(f\"Inserted patient: {patient_data.get('patient_id')}\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Failed to insert patient {patient_data.get('patient_id')}: {e}\")\n",
    "        else:\n",
    "            print(f\"Failed to insert patient {patient_data.get('patient_id')}: {e}\")\n",
    "        raise\n",
    "   \n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    Main_logger.debug(\"All patients, addresses, and telecoms inserted into Postgres successfully.\")\n",
    "    #try:\n",
    "    ''''cur.execute(\"\"\" INSERT INTO fhir_staging.patients_fhir_raw \n",
    "                (patient_id, \n",
    "                first_name, \n",
    "                last_name, \n",
    "                prefix, \n",
    "                gender, \n",
    "                birth_date, \n",
    "                deceased_date_time, \n",
    "                ssn, drivers_license, \n",
    "                passport, \n",
    "                marital_status, \n",
    "                race, \n",
    "                ethnicity, \n",
    "                birth_place, \n",
    "                resource) \n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) \n",
    "                ON CONFLICT (patient_id) DO NOTHING \"\"\", \n",
    "                ( patient_data[\"patient_id\"], \n",
    "                    patient_data.get(\"first_name\"),\n",
    "                    patient_data.get(\"last_name\"), \n",
    "                    patient_data.get(\"prefix\"), \n",
    "                    patient_data.get(\"gender\"), \n",
    "                    patient_data.get(\"birth_date\"), \n",
    "                    patient_data.get(\"deceased_date_time\"), \n",
    "                    patient_data.get(\"ssn\"), \n",
    "                    patient_data.get(\"drivers_license\"), \n",
    "                    patient_data.get(\"passport\"), \n",
    "                    patient_data.get(\"marital_status\"), \n",
    "                    Json(patient_data.get(\"race\", {})), \n",
    "                    Json(patient_data.get(\"ethnicity\", {})), \n",
    "                    patient_data.get(\"birth_place\"), \n",
    "                    Json(patient_data.get(\"resource\", {})) ) )'''\n",
    "        #if logger:\n",
    "        #    Main_logger.debug(f\"Inserted patient: {patient_data.get('patient_id')}\")\n",
    "    #except Exception as e:\n",
    "      #  if logger:\n",
    "    #Main_logger.error(f\"Failed to insert patient {patient_data.get('patient_id')}: {e}\")\n",
    "       # else:\n",
    "    #print(f\"Failed to insert patient {patient_data.get('patient_id')}: {e}\")\n",
    "        #raise\n",
    "    \n",
    "    columns = [\n",
    "        \"patient_id\",\n",
    "        \"first_name\",\n",
    "        \"last_name\",\n",
    "        \"prefix\",\n",
    "        \"gender\",\n",
    "        \"birth_date\",\n",
    "        \"deceased_date_time\",\n",
    "        \"ssn\",\n",
    "        \"drivers_license\",\n",
    "        \"passport\",\n",
    "        \"marital_status\",\n",
    "        \"race\",\n",
    "        \"ethnicity\",\n",
    "        \"birth_place\",\n",
    "        \"resource\"\n",
    "    ]\n",
    "\n",
    "    # Define which columns are JSONB\n",
    "    jsonb_columns = {\"race\", \"ethnicity\", \"birth_place\", \"resource\"}\n",
    "\n",
    "\n",
    "    values = []\n",
    "    for col in columns:\n",
    "        val = patient_data.get(col)\n",
    "        if col in jsonb_columns:\n",
    "            if isinstance(val, (dict, list)):\n",
    "                val = json.dumps(val)\n",
    "            elif isinstance(val, str):\n",
    "                # Wrap strings as JSON object {\"text\": \"...\"} for JSONB\n",
    "                val = json.dumps({\"text\": val})\n",
    "            else:\n",
    "                val = json.dumps({})\n",
    "        values.append(val)\n",
    "    \n",
    "    # Build the SQL query safely using psycopg2.sql\n",
    "    query = sql.SQL(\"\"\"\n",
    "        INSERT INTO fhir_staging.patients_fhir_raw ({fields})\n",
    "        VALUES ({placeholders})\n",
    "        ON CONFLICT (patient_id) DO NOTHING\n",
    "    \"\"\").format(\n",
    "        fields=sql.SQL(', ').join(map(sql.Identifier, columns)),\n",
    "        placeholders=sql.SQL(', ').join(sql.Placeholder() * len(columns))\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        cur.execute(query, values)\n",
    "        if logger:\n",
    "            logger.debug(f\"Inserted patient: {patient_data.get('patient_id')}\")\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Failed to insert patient {patient_data.get('patient_id')}: {e}\")\n",
    "        else:\n",
    "            print(f\"Failed to insert patient {patient_data.get('patient_id')}: {e}\")\n",
    "        raise\n",
    "   \n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    Main_logger.debug(\"All patients, addresses, and telecoms inserted into Postgres successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c221527",
   "metadata": {},
   "source": [
    "This is the parser for patients and inserts data into a postgres database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40020d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addy: 838 Vandervort Loaf Apt 89\n"
     ]
    }
   ],
   "source": [
    "###Refactor 09-03-2025 Testing with database\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "from psycopg2.extras import Json\n",
    "from psycopg2 import sql\n",
    "import logging\n",
    "\n",
    "# --- Helpers ---\n",
    "def clean_nested(data):\n",
    "    \"\"\"Recursively replace empty dicts/lists with None.\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        if not data:\n",
    "            return None\n",
    "        return {k: clean_nested(v) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        if not data:\n",
    "            return None\n",
    "        return [clean_nested(v) for v in data]\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def safe_get(d, *keys, default=None):\n",
    "    \"\"\"Safely extract nested values from a dict.\"\"\"\n",
    "    for k in keys:\n",
    "        if isinstance(d, dict):\n",
    "            d = d.get(k, default)\n",
    "        else:\n",
    "            return default\n",
    "    return d\n",
    "\n",
    "def extract_identifier(resource, code):\n",
    "    \"\"\"Extract an identifier of a given type code (SS, DL, PPN).\"\"\"\n",
    "    for ident in resource.get(\"identifier\", []):\n",
    "        type_code = safe_get(ident.get(\"type\", {}).get(\"coding\", [{}])[0], \"code\")\n",
    "        if type_code == code:\n",
    "            return ident.get(\"value\")\n",
    "    return None\n",
    "\n",
    "def extract_extensions(resource, url):\n",
    "    \"\"\"Extract FHIR extensions matching a URL.\"\"\"\n",
    "    result = {}\n",
    "    for ext in resource.get(\"extension\", []) or []:\n",
    "        if ext.get(\"url\") == url:\n",
    "            for inner in ext.get(\"extension\", []) or []:\n",
    "                key = inner.get(\"url\")\n",
    "                value = inner.get(\"valueString\") or safe_get(inner, \"valueCoding\", \"display\")\n",
    "                if key:\n",
    "                    result[key] = value\n",
    "    return result if result else None\n",
    "\n",
    "def extract_geolocation(address):\n",
    "    \"\"\"Extract latitude/longitude from a FHIR address extension.\"\"\"\n",
    "    latitude = longitude = None\n",
    "    for ext in address.get(\"extension\", []) or []:\n",
    "        if ext.get(\"url\") == \"http://hl7.org/fhir/StructureDefinition/geolocation\":\n",
    "            for geo in ext.get(\"extension\", []) or []:\n",
    "                if geo.get(\"url\") == \"latitude\":\n",
    "                    latitude = geo.get(\"valueDecimal\")\n",
    "                elif geo.get(\"url\") == \"longitude\":\n",
    "                    longitude = geo.get(\"valueDecimal\")\n",
    "    return latitude, longitude\n",
    "\n",
    "def extract_birth_place(resource):\n",
    "    \"\"\"Return formatted birth_place string or None.\"\"\"\n",
    "    extensions = resource.get(\"extension\", []) or []\n",
    "    birth_place = next(\n",
    "        (ext.get(\"valueAddress\") for ext in extensions \n",
    "         if ext.get(\"url\") == \"http://hl7.org/fhir/StructureDefinition/patient-birthPlace\"),\n",
    "        None\n",
    "    )\n",
    "    if birth_place:\n",
    "        parts = [birth_place.get(\"city\"), birth_place.get(\"state\"), birth_place.get(\"country\")]\n",
    "        return \", \".join([p for p in parts if p]) or None\n",
    "    return None\n",
    "\n",
    "def extract_addresses(resource):\n",
    "    \"\"\"Return list of addresses with geolocation if available.\"\"\"\n",
    "    addresses = []\n",
    "    for addr in resource.get(\"address\", []) or []:\n",
    "        lat, lon = extract_geolocation(addr)\n",
    "        addresses.append({\n",
    "            \"line\": addr.get(\"line\"),\n",
    "            \"city\": addr.get(\"city\"),\n",
    "            \"state\": addr.get(\"state\"),\n",
    "            \"postal_code\": addr.get(\"postalCode\"),\n",
    "            \"country\": addr.get(\"country\"),\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon\n",
    "        })\n",
    "    return addresses if addresses else None\n",
    "\n",
    "def extract_telecoms(resource):\n",
    "    \"\"\"Return list of telecoms (phone/email/etc).\"\"\"\n",
    "    telecoms_list = []\n",
    "    for t in resource.get(\"telecom\", []) or []:\n",
    "        telecoms_list.append({\n",
    "            \"system\": t.get(\"system\"),\n",
    "            \"value\": t.get(\"value\"),\n",
    "            \"use\": t.get(\"use\"),\n",
    "            \"extension\": t.get(\"extension\")  # e.g., US Core Direct\n",
    "        })\n",
    "    return telecoms_list if telecoms_list else None\n",
    "\n",
    "def extract_text_or_display(obj, text_key=\"text\", coding_key=\"coding\", category_key=\"ombCategory\"):\n",
    "    \"\"\"\n",
    "    Extracts a human-readable string from a FHIR object like maritalStatus, race, ethnicity.\n",
    "    Priority: text -> coding.display -> ombCategory.display\n",
    "    \"\"\"\n",
    "    if not isinstance(obj, dict):\n",
    "        return None\n",
    "\n",
    "    # Prefer the plain \"text\" field\n",
    "    if text_key in obj and obj[text_key]:\n",
    "        return obj[text_key]\n",
    "\n",
    "    # Fallback to first coding.display\n",
    "    if coding_key in obj and isinstance(obj[coding_key], list) and obj[coding_key]:\n",
    "        return obj[coding_key][0].get(\"display\")\n",
    "\n",
    "    # Fallback to ombCategory.display (if race/ethnicity style)\n",
    "    if category_key in obj and isinstance(obj[category_key], dict):\n",
    "        return obj[category_key].get(\"display\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def extract_human_readable(obj):\n",
    "    \"\"\"\n",
    "    Return a human-readable string from common FHIR shapes:\n",
    "      - CodeableConcept: {'coding':[...], 'text': '...'}\n",
    "      - Coding: {'system':..., 'code':..., 'display':...}\n",
    "      - US Core race/ethnicity extension shapes (nested 'extension' arrays)\n",
    "      - valueCodeableConcept / valueCoding / valueString\n",
    "      - Lists of any of the above (returns first non-empty)\n",
    "    Priority (rough): text / valueString -> coding[0].display -> coding[0].code -> valueCoding.display -> ombCategory.display\n",
    "    \"\"\"\n",
    "    # quick guards\n",
    "    if obj is None:\n",
    "        return None\n",
    "    if isinstance(obj, str):\n",
    "        return obj\n",
    "\n",
    "    # lists: return first non-None result\n",
    "    if isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            v = extract_human_readable(item)\n",
    "            if v:\n",
    "                return v\n",
    "        return None\n",
    "\n",
    "    # dict handling\n",
    "    if isinstance(obj, dict):\n",
    "        # direct fields\n",
    "        if obj.get(\"text\"):\n",
    "            return obj.get(\"text\")\n",
    "        if obj.get(\"display\"):\n",
    "            return obj.get(\"display\")\n",
    "        if obj.get(\"valueString\"):\n",
    "            return obj.get(\"valueString\")\n",
    "\n",
    "        # valueCoding\n",
    "        vc = obj.get(\"valueCoding\") or obj.get(\"valueCoding\")\n",
    "        if isinstance(vc, dict):\n",
    "            return vc.get(\"display\") or vc.get(\"code\")\n",
    "\n",
    "        # valueCodeableConcept\n",
    "        vcc = obj.get(\"valueCodeableConcept\")\n",
    "        if vcc is not None:\n",
    "            return extract_human_readable(vcc)\n",
    "\n",
    "        # coding list (CodeableConcept)\n",
    "        coding = obj.get(\"coding\")\n",
    "        if isinstance(coding, list) and coding:\n",
    "            first = coding[0]\n",
    "            if isinstance(first, dict):\n",
    "                return first.get(\"display\") or first.get(\"code\")\n",
    "\n",
    "        # ombCategory (US Core race/ethnicity compact shape)\n",
    "        omb = obj.get(\"ombCategory\")\n",
    "        if isinstance(omb, dict):\n",
    "            return omb.get(\"display\") or omb.get(\"code\")\n",
    "\n",
    "        # Some FHIR extensions use 'extension' arrays (e.g., US Core race/ethnicity)\n",
    "        if isinstance(obj.get(\"extension\"), list):\n",
    "            # First, try the common nested pattern where an extension contains a nested 'extension' list\n",
    "            for ext in obj[\"extension\"]:\n",
    "                # If ext directly has a valueString / valueCoding / valueCodeableConcept, use it\n",
    "                if isinstance(ext, dict):\n",
    "                    if ext.get(\"valueString\"):\n",
    "                        return ext.get(\"valueString\")\n",
    "                    if ext.get(\"valueCoding\") and isinstance(ext.get(\"valueCoding\"), dict):\n",
    "                        return ext[\"valueCoding\"].get(\"display\") or ext[\"valueCoding\"].get(\"code\")\n",
    "                    if ext.get(\"valueCodeableConcept\"):\n",
    "                        v = extract_human_readable(ext[\"valueCodeableConcept\"])\n",
    "                        if v:\n",
    "                            return v\n",
    "                    # If ext contains its own nested 'extension' list (the US Core pattern)\n",
    "                    if isinstance(ext.get(\"extension\"), list):\n",
    "                        # try to find 'text' sub-extension first\n",
    "                        for sub in ext[\"extension\"]:\n",
    "                            if sub.get(\"url\") == \"text\" and sub.get(\"valueString\"):\n",
    "                                return sub.get(\"valueString\")\n",
    "                        # try to find 'ombCategory' or similar sub-extension with valueCoding\n",
    "                        for sub in ext[\"extension\"]:\n",
    "                            if sub.get(\"url\") in (\"ombCategory\", \"race\", \"ethnicity\") and sub.get(\"valueCoding\"):\n",
    "                                vc = sub.get(\"valueCoding\")\n",
    "                                if isinstance(vc, dict):\n",
    "                                    return vc.get(\"display\") or vc.get(\"code\")\n",
    "                        # fallback: recurse into each sub-extension\n",
    "                        for sub in ext[\"extension\"]:\n",
    "                            v = extract_human_readable(sub)\n",
    "                            if v:\n",
    "                                return v\n",
    "\n",
    "            # If we didn't return yet, try recursing into each top-level extension element\n",
    "            for ext in obj[\"extension\"]:\n",
    "                v = extract_human_readable(ext)\n",
    "                if v:\n",
    "                    return v\n",
    "\n",
    "        # fallback: sometimes keys like 'category' or other nested dicts contain display/code\n",
    "        for key in (\"category\",):\n",
    "            val = obj.get(key)\n",
    "            if isinstance(val, dict):\n",
    "                return val.get(\"display\") or val.get(\"code\")\n",
    "\n",
    "    # no match\n",
    "    return None\n",
    "\n",
    "def parse_race(resource):\n",
    "    \"\"\"\n",
    "    Extracts the patient's race from a FHIR Patient resource.\n",
    "    \n",
    "    Returns the display string or None if not found.\n",
    "    \"\"\"\n",
    "    if \"extension\" not in resource:\n",
    "        return None\n",
    "\n",
    "    for ext in resource[\"extension\"]:\n",
    "        if ext.get(\"url\") == \"http://hl7.org/fhir/us/core/StructureDefinition/us-core-race\":\n",
    "            for sub_ext in ext.get(\"extension\", []):\n",
    "                if sub_ext.get(\"url\") == \"ombCategory\":\n",
    "                    return sub_ext.get(\"valueCoding\", {}).get(\"display\")\n",
    "    return None\n",
    "\n",
    "def parse_ethnicity(patient_json):\n",
    "    ethnicity = None\n",
    "    if \"extension\" in patient_json:\n",
    "        for ext in patient_json[\"extension\"]:\n",
    "            if ext.get(\"url\") == \"http://hl7.org/fhir/us/core/StructureDefinition/us-core-ethnicity\":\n",
    "                for sub_ext in ext.get(\"extension\", []):\n",
    "                    if sub_ext.get(\"url\") == \"ombCategory\":\n",
    "                        ethnicity = sub_ext.get(\"valueCoding\", {}).get(\"display\")\n",
    "    return ethnicity\n",
    "\n",
    "import json\n",
    "import psycopg2\n",
    "# Create a logger\n",
    "import logging\n",
    "def logger(name, log_file, level=logging.DEBUG):\n",
    "    \"\"\"Create a dedicated logger for a parser method.\"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    \n",
    "    # Avoid adding multiple handlers if logger already exists\n",
    "    if not logger.handlers:\n",
    "        file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')\n",
    "        file_handler.setLevel(level)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "def insert_patients(patients_list, conn):\n",
    "    \"\"\"\n",
    "        Inserts patients, addresses, and telecoms into Postgres.\n",
    "        Expects a list of patient dicts (already parsed).\n",
    "    \"\"\"\n",
    "    insertPatients_logger = logger(\n",
    "        \"INS_patients\", \n",
    "        \"C:\\\\Users\\\\tonim\\\\Downloads\\\\output\\\\fhir\\\\ins_patients_log\"\n",
    "    )\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Queries\n",
    "    patient_query = \"\"\"\n",
    "    INSERT INTO fhir_staging.patients_fhir_raw (\n",
    "        patient_id, first_name, last_name, prefix, gender, birth_date, deceased_date_time,\n",
    "        ssn, drivers_license, passport, marital_status, race, ethnicity, birth_place, resource\n",
    "    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    ON CONFLICT (patient_id) DO NOTHING\n",
    "    \"\"\"\n",
    "\n",
    "    address_query = \"\"\"\n",
    "    INSERT INTO fhir_staging.patient_addresses (\n",
    "        patient_id, line, city, state, postal_code, country, latitude, longitude\n",
    "    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "\n",
    "    telecom_query = \"\"\"\n",
    "    INSERT INTO fhir_staging.patient_telecoms (\n",
    "        patient_id, system, value, use, extension\n",
    "    ) VALUES (%s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "\n",
    "    for patient in patients_list:\n",
    "        # Insert patient\n",
    "        cur.execute(patient_query, (\n",
    "            patient.get(\"patient_id\"),\n",
    "            patient.get(\"first_name\"),\n",
    "            patient.get(\"last_name\"),\n",
    "            patient.get(\"prefix\"),\n",
    "            patient.get(\"gender\"),\n",
    "            patient.get(\"birth_date\"),\n",
    "            patient.get(\"deceased_date_time\"),\n",
    "            patient.get(\"ssn\"),\n",
    "            patient.get(\"drivers_license\"),\n",
    "            patient.get(\"passport\"),\n",
    "            patient.get(\"marital_status\"),\n",
    "            patient.get(\"race\"),\n",
    "            patient.get(\"ethnicity\"),\n",
    "            patient.get(\"birth_place\"),\n",
    "            json.dumps(patient.get(\"resource\")) if patient.get(\"resource\") else None\n",
    "        ))\n",
    "\n",
    "        # Insert addresses\n",
    "        for addr in patient.get(\"addresses\", []):\n",
    "            #print(f\"address: {addr}\")\n",
    "            #lines = addr.get(\"line\", [])\n",
    "            addy = \"\"\n",
    "            lines = addr.get(\"line\")\n",
    "\n",
    "        # Normalize lines into a list of strings\n",
    "        if lines is None:\n",
    "            lines = []\n",
    "        elif isinstance(lines, str):\n",
    "            lines = [lines]\n",
    "        elif isinstance(lines, list):\n",
    "            # Extract strings if the list contains dicts\n",
    "            lines = [line if isinstance(line, str) else line.get(\"line\", \"\") for line in lines]\n",
    "\n",
    "        # Join with comma + space\n",
    "        addy = \", \".join([line for line in lines if line])\n",
    "        print(f\"addy: {addy}\")\n",
    "        city = addr.get(\"city\")\n",
    "        state = addr.get(\"state\")\n",
    "        postal_code = addr.get(\"postalCode\")\n",
    "        country = addr.get(\"country\")\n",
    "        #latitude = addr.get(\"latitude\")\n",
    "        #longitude = addr.get(\"longitude\")\n",
    "        latitude = float(addr.get(\"latitude\")) if addr.get(\"latitude\") is not None else None\n",
    "        longitude = float(addr.get(\"longitude\")) if addr.get(\"longitude\") is not None else None\n",
    "\n",
    "        cur.execute(address_query, (\n",
    "            patient.get(\"patient_id\"),\n",
    "            lines,\n",
    "            city,\n",
    "            state,\n",
    "            postal_code,\n",
    "            country,\n",
    "            latitude,\n",
    "            longitude\n",
    "        ))\n",
    "\n",
    "        # Insert telecoms\n",
    "        for tel in patient.get(\"telecoms\", []):\n",
    "            system = tel.get(\"system\")\n",
    "            value = tel.get(\"value\")\n",
    "            use = tel.get(\"use\")\n",
    "            extension = tel.get(\"extension\")  # this can be a list/dict\n",
    "            #insertPatients_logger(f\"extension: {extension}\")\n",
    "\n",
    "            # Convert extension to JSON string if it exists\n",
    "            extension_json = json.dumps(extension) if extension else None\n",
    "            \n",
    "            cur.execute(telecom_query, (\n",
    "                patient.get(\"patient_id\"),\n",
    "                system,\n",
    "                value,\n",
    "                use,\n",
    "                extension\n",
    "            ))\n",
    "\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "\n",
    "\n",
    "\n",
    "def parse_patient_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if \"entry\" not in data:\n",
    "        return []\n",
    "\n",
    "    patients = []\n",
    "    for entry in data[\"entry\"]:\n",
    "        resource = entry.get(\"resource\", {})\n",
    "        if resource.get(\"resourceType\") != \"Patient\":\n",
    "            continue\n",
    "        \n",
    "        id = resource.get(\"id\")\n",
    "        Main_logger.debug(f\"id: {id}\")\n",
    "        names = resource.get(\"name\", [])\n",
    "\n",
    "        name = names[0] if names else {}\n",
    "        given_name = safe_get(name.get(\"given\", [None])[0])\n",
    "        Main_logger.debug(f\"given_name: {given_name}\")\n",
    "\n",
    "        last_name = name.get(\"family\")\n",
    "        Main_logger.debug(f\"last_name: {last_name}\")\n",
    "        prefix = safe_get(name.get(\"prefix\", [None])[0])\n",
    "        Main_logger.debug(f\"prefix: {prefix}\")\n",
    "        gender = resource.get(\"gender\")\n",
    "        Main_logger.debug(f\"gender: {gender}\")\n",
    "        birth_date = resource.get(\"birthDate\")\n",
    "        Main_logger.debug(f\"birth_date: {birth_date}\")\n",
    "        deceased_date_time = resource.get(\"deceasedDateTime\")\n",
    "        Main_logger.debug(f\"deceased_date: {deceased_date_time}\")\n",
    "        ssn = extract_identifier(resource, \"SS\")\n",
    "        Main_logger.debug(f\"ssn: {ssn}\")\n",
    "        drivers_license = extract_identifier(resource, \"DL\")\n",
    "        Main_logger.debug(f\"drivers_license: {drivers_license}\")\n",
    "        passport = extract_identifier(resource, \"PPN\")\n",
    "        Main_logger.debug(f\"passport: {passport}\")\n",
    "    \n",
    "        #ms = resource.get(\"maritalStatus\") or {}\n",
    "        #marital_status = ms.get(\"text\") or ((ms.get(\"coding\") or [{}])[0].get(\"display\"))\n",
    "        #Main_logger.debug(f\"marital_status: {marital_status}\")\n",
    "        #race = extract_extensions(resource, \"http://hl7.org/fhir/us/core/StructureDefinition/us-core-race\")\n",
    "        #Main_logger.debug(f\"race: {race}\")\n",
    "        #ethnicity =extract_extensions(resource, \"http://hl7.org/fhir/us/core/StructureDefinition/us-core-ethnicity\")\n",
    "\n",
    "\n",
    "        #Main_logger.debug(f\"ethicity: {ethnicity}\")\n",
    "\n",
    "        marital_status = extract_human_readable(resource.get(\"maritalStatus\")) \\\n",
    "                 or extract_human_readable(resource.get(\"extension\"))\n",
    "        \n",
    "        #race = extract_human_readable(resource.get(\"race\")) or extract_human_readable(resource.get(\"extension\"))\n",
    "        race = parse_race(resource)\n",
    "        #ethnicity = extract_human_readable(resource.get(\"ethnicity\")) or extract_human_readable(resource.get(\"extension\"))\n",
    "        ethnicity = parse_ethnicity(resource)\n",
    "\n",
    "        #race = extract_text_or_display(resource.get(\"race\"))\n",
    "        Main_logger.debug(f\"race: {race}\")\n",
    "        #ethnicity = extract_text_or_display(resource.get(\"ethnicity\"))\n",
    "        Main_logger.debug(f\"ethnicity: {ethnicity}\")\n",
    "        #marital_status = extract_text_or_display(resource.get(\"maritalStatus\"))\n",
    "        Main_logger.debug(f\"marital_status: {marital_status}\")\n",
    "            \n",
    "        #extensions = safe_get(resource, \"extension\", default=[])\n",
    "        #address = extensions.get(\"address\",[0]).get(\"line\",[])\n",
    "        #address = resource.get(\"address\",[])\n",
    "        #telecoms = resource.get(\"telecom\", [])\n",
    "        \n",
    "        birth_place_str = extract_birth_place(resource)\n",
    "        Main_logger.info(f\"birth_place_str: {birth_place_str}\")\n",
    "\n",
    "        addresses = extract_addresses(resource)\n",
    "        telecoms = extract_telecoms(resource)\n",
    "        \n",
    "\n",
    "        patients.append({\n",
    "            \"patient_id\": resource.get(\"id\"),\n",
    "            \"first_name\": given_name,\n",
    "            \"last_name\": last_name,\n",
    "            \"prefix\": prefix,\n",
    "            \"gender\": resource.get(\"gender\"),\n",
    "            \"birth_date\": resource.get(\"birthDate\"),\n",
    "            \"deceased_date_time\": resource.get(\"deceasedDateTime\"),\n",
    "            \"ssn\": extract_identifier(resource, \"SS\"),\n",
    "            \"drivers_license\": extract_identifier(resource, \"DL\"),\n",
    "            \"passport\": extract_identifier(resource, \"PPN\"),\n",
    "            \"marital_status\": marital_status,\n",
    "            \"race\": race,\n",
    "            \"ethnicity\": ethnicity,\n",
    "            \"birth_place\": birth_place_str,\n",
    "            \"addresses\": addresses,\n",
    "            \"telecoms\": telecoms,\n",
    "            \"resource\": json.dumps(resource) if resource else None\n",
    "        })\n",
    "\n",
    "    return patients\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    Main_logger = logger(\n",
    "        \"Main\", \n",
    "        \"C:\\\\Users\\\\tonim\\\\Downloads\\\\output\\\\fhir\\\\Main_log\"\n",
    "    )\n",
    "    #Main_logger = logging.getLogger(\"FHIR_ETL\")\n",
    "    #Main_logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    '''if not Main_logger.handlers:\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(logging.DEBUG)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        ch.setFormatter(formatter)\n",
    "        Main_logger.addHandler(ch)'''\n",
    "\n",
    "    test_patient_data = {\n",
    "    \"entry\": [\n",
    "        {\n",
    "            \"fullUrl\": \"urn:uuid:4ee53233-844d-50b5-32c4-eff7de5fbfdd\",\n",
    "            \"resource\": {\n",
    "                \"resourceType\": \"Patient\",\n",
    "                \"id\": \"4ee53233-844d-50b5-32c4-eff7de5fbfdd\",\n",
    "                \"meta\": {\n",
    "                    \"profile\": [\"http://hl7.org/fhir/us/core/StructureDefinition/us-core-patient\"]\n",
    "                },\n",
    "                \"text\": {\n",
    "                    \"status\": \"generated\",\n",
    "                    \"div\": \"<div xmlns=\\\"http://www.w3.org/1999/xhtml\\\">Generated by <a href=\\\"https://github.com/synthetichealth/synthea\\\">Synthea</a>.Version identifier: 3a65f56. Person seed: -7800982323046788640 Population seed: 1755716738658</div>\"\n",
    "                },\n",
    "                \"extension\": [\n",
    "                    {\n",
    "                        \"url\": \"http://hl7.org/fhir/us/core/StructureDefinition/us-core-race\",\n",
    "                        \"extension\": [\n",
    "                            {\n",
    "                                \"url\": \"ombCategory\",\n",
    "                                \"valueCoding\": {\n",
    "                                    \"system\": \"urn:oid:2.16.840.1.113883.6.238\",\n",
    "                                    \"code\": \"2054-5\",\n",
    "                                    \"display\": \"Black or African American\"\n",
    "                                }\n",
    "                            },\n",
    "                            {\n",
    "                                \"url\": \"text\",\n",
    "                                \"valueString\": \"Black or African American\"\n",
    "                            }\n",
    "                        ]\n",
    "                    },\n",
    "                    {\n",
    "                        \"url\": \"http://hl7.org/fhir/us/core/StructureDefinition/us-core-ethnicity\",\n",
    "                        \"extension\": [\n",
    "                            {\n",
    "                                \"url\": \"ombCategory\",\n",
    "                                \"valueCoding\": {\n",
    "                                    \"system\": \"urn:oid:2.16.840.1.113883.6.238\",\n",
    "                                    \"code\": \"2186-5\",\n",
    "                                    \"display\": \"Not Hispanic or Latino\"\n",
    "                                }\n",
    "                            },\n",
    "                            {\n",
    "                                \"url\": \"text\",\n",
    "                                \"valueString\": \"Not Hispanic or Latino\"\n",
    "                            }\n",
    "                        ]\n",
    "                    },\n",
    "                    {\n",
    "                        \"url\": \"http://hl7.org/fhir/StructureDefinition/patient-mothersMaidenName\",\n",
    "                        \"valueString\": \"Candice681 Marks830\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"url\": \"http://hl7.org/fhir/us/core/StructureDefinition/us-core-birthsex\",\n",
    "                        \"valueCode\": \"F\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"url\": \"http://hl7.org/fhir/StructureDefinition/patient-birthPlace\",\n",
    "                        \"valueAddress\": {\n",
    "                            \"city\": \"Seekonk\",\n",
    "                            \"state\": \"Massachusetts\",\n",
    "                            \"country\": \"US\"\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"url\": \"http://synthetichealth.github.io/synthea/disability-adjusted-life-years\",\n",
    "                        \"valueDecimal\": 21.40718108461066\n",
    "                    },\n",
    "                    {\n",
    "                        \"url\": \"http://synthetichealth.github.io/synthea/quality-adjusted-life-years\",\n",
    "                        \"valueDecimal\": 48.592818915389344\n",
    "                    }\n",
    "                ],\n",
    "                \"identifier\": [\n",
    "                    {\n",
    "                        \"system\": \"https://github.com/synthetichealth/synthea\",\n",
    "                        \"value\": \"4ee53233-844d-50b5-32c4-eff7de5fbfdd\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": {\n",
    "                            \"coding\": [\n",
    "                                {\n",
    "                                    \"system\": \"http://terminology.hl7.org/CodeSystem/v2-0203\",\n",
    "                                    \"code\": \"MR\",\n",
    "                                    \"display\": \"Medical Record Number\"\n",
    "                                }\n",
    "                            ],\n",
    "                            \"text\": \"Medical Record Number\"\n",
    "                        },\n",
    "                        \"system\": \"http://hospital.smarthealthit.org\",\n",
    "                        \"value\": \"4ee53233-844d-50b5-32c4-eff7de5fbfdd\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": {\n",
    "                            \"coding\": [\n",
    "                                {\n",
    "                                    \"system\": \"http://terminology.hl7.org/CodeSystem/v2-0203\",\n",
    "                                    \"code\": \"SS\",\n",
    "                                    \"display\": \"Social Security Number\"\n",
    "                                }\n",
    "                            ],\n",
    "                            \"text\": \"Social Security Number\"\n",
    "                        },\n",
    "                        \"system\": \"http://hl7.org/fhir/sid/us-ssn\",\n",
    "                        \"value\": \"999-81-1696\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": {\n",
    "                            \"coding\": [\n",
    "                                {\n",
    "                                    \"system\": \"http://terminology.hl7.org/CodeSystem/v2-0203\",\n",
    "                                    \"code\": \"DL\",\n",
    "                                    \"display\": \"Driver's license number\"\n",
    "                                }\n",
    "                            ],\n",
    "                            \"text\": \"Driver's license number\"\n",
    "                        },\n",
    "                        \"system\": \"urn:oid:2.16.840.1.113883.4.3.25\",\n",
    "                        \"value\": \"S99934087\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": {\n",
    "                            \"coding\": [\n",
    "                                {\n",
    "                                    \"system\": \"http://terminology.hl7.org/CodeSystem/v2-0203\",\n",
    "                                    \"code\": \"PPN\",\n",
    "                                    \"display\": \"Passport Number\"\n",
    "                                }\n",
    "                            ],\n",
    "                            \"text\": \"Passport Number\"\n",
    "                        },\n",
    "                        \"system\": \"http://hl7.org/fhir/sid/passport-USA\",\n",
    "                        \"value\": \"X17415579X\"\n",
    "                    }\n",
    "                ],\n",
    "                \"name\": [\n",
    "                    {\n",
    "                        \"use\": \"official\",\n",
    "                        \"family\": \"Sipes176\",\n",
    "                        \"given\": [\"Armida530\"],\n",
    "                        \"prefix\": [\"Mrs.\"]\n",
    "                    },\n",
    "                    {\n",
    "                        \"use\": \"maiden\",\n",
    "                        \"family\": \"McKenzie376\",\n",
    "                        \"given\": [\"Armida530\"],\n",
    "                        \"prefix\": [\"Mrs.\"]\n",
    "                    }\n",
    "                ],\n",
    "                \"telecom\": [\n",
    "                    {\n",
    "                        \"system\": \"phone\",\n",
    "                        \"value\": \"555-147-8220\",\n",
    "                        \"use\": \"home\"\n",
    "                    }\n",
    "                ],\n",
    "                \"gender\": \"female\",\n",
    "                \"birthDate\": \"1954-04-18\",\n",
    "                \"address\": [\n",
    "                    {\n",
    "                        \"extension\": [\n",
    "                            {\n",
    "                                \"url\": \"http://hl7.org/fhir/StructureDefinition/geolocation\",\n",
    "                                \"extension\": [\n",
    "                                    {\"url\": \"latitude\", \"valueDecimal\": 41.91817130004856},\n",
    "                                    {\"url\": \"longitude\", \"valueDecimal\": -70.8768795282484}\n",
    "                                ]\n",
    "                            }\n",
    "                        ],\n",
    "                        \"line\": [\"838 Vandervort Loaf Apt 89\"],\n",
    "                        \"city\": \"Middleborough\",\n",
    "                        \"state\": \"MA\",\n",
    "                        \"postalCode\": \"00000\",\n",
    "                        \"country\": \"US\"\n",
    "                    }\n",
    "                ],\n",
    "                \"maritalStatus\": {\n",
    "                    \"coding\": [\n",
    "                        {\n",
    "                            \"system\": \"http://terminology.hl7.org/CodeSystem/v3-MaritalStatus\",\n",
    "                            \"code\": \"M\",\n",
    "                            \"display\": \"Married\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"text\": \"Married\"\n",
    "                },\n",
    "                \"multipleBirthBoolean\": False,\n",
    "                \"communication\": [\n",
    "                    {\n",
    "                        \"language\": {\n",
    "                            \"coding\": [\n",
    "                                {\n",
    "                                    \"system\": \"urn:ietf:bcp:47\",\n",
    "                                    \"code\": \"en-US\",\n",
    "                                    \"display\": \"English (United States)\"\n",
    "                                }\n",
    "                            ],\n",
    "                            \"text\": \"English (United States)\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"request\": {\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"Patient\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "with open(\"C:\\\\Users\\\\tonim\\\\Downloads\\\\output\\\\test_patient.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(test_patient_data, f, indent=2)\n",
    "test_file = r\"C:\\Users\\tonim\\Downloads\\output\\test_patient.json\"\n",
    "#test_file = r\"C:\\Users\\tonim\\Downloads\\output\\fhir\\Patients\\Floyd420_Streich926_42f4db2f-b049-c9a1-a961-7a944ea72e48.json\"\n",
    "#patients_output_file = r\"C:\\Users\\tonim\\Downloads\\output\\parquet\\patients.parquet\"\n",
    "# \n",
    "conn = psycopg2.connect(\"dbname=FHIR_staging user=postgres password=new_password host=localhost\")   \n",
    "patients_data = [clean_nested(p) for p in (parse_patient_file(test_file))]\n",
    "\n",
    "insert_patients(patients_data, conn)\n",
    "\n",
    "if not patients_data:\n",
    "        Main_logger.debug(\"No patient records found.\")\n",
    "else:\n",
    "        Main_logger.debug(f\"Parsed {len(patients_data)} patients.\")\n",
    "\n",
    "#conn = psycopg2.connect(\"dbname=FHIR_staging user=youruser password=new_password host=localhost\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db10b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "\n",
    "# --- Helpers ---\n",
    "def clean_nested(data):\n",
    "    \"\"\"Recursively replace empty dicts/lists with None.\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        if not data:\n",
    "            return None\n",
    "        return {k: clean_nested(v) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        if not data:\n",
    "            return None\n",
    "        return [clean_nested(v) for v in data]\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def safe_get(d, *keys, default=None):\n",
    "    \"\"\"Safely extract nested values from a dict.\"\"\"\n",
    "    for k in keys:\n",
    "        if isinstance(d, dict):\n",
    "            d = d.get(k, default)\n",
    "        else:\n",
    "            return default\n",
    "    return d\n",
    "\n",
    "def extract_identifier(resource, code):\n",
    "    \"\"\"Extract an identifier of a given type code (SS, DL, PPN).\"\"\"\n",
    "    for ident in resource.get(\"identifier\", []):\n",
    "        type_code = safe_get(ident.get(\"type\", {}).get(\"coding\", [{}])[0], \"code\")\n",
    "        if type_code == code:\n",
    "            return ident.get(\"value\")\n",
    "    return None\n",
    "\n",
    "def extract_extensions(resource, url):\n",
    "    \"\"\"Extract FHIR extensions matching a URL.\"\"\"\n",
    "    result = {}\n",
    "    for ext in resource.get(\"extension\", []) or []:\n",
    "        if ext.get(\"url\") == url:\n",
    "            for inner in ext.get(\"extension\", []) or []:\n",
    "                key = inner.get(\"url\")\n",
    "                value = inner.get(\"valueString\") or safe_get(inner, \"valueCoding\", \"display\")\n",
    "                if key:\n",
    "                    result[key] = value\n",
    "    return result if result else None\n",
    "\n",
    "def extract_geolocation(address):\n",
    "    \"\"\"Extract latitude/longitude from a FHIR address extension.\"\"\"\n",
    "    latitude = longitude = None\n",
    "    for ext in address.get(\"extension\", []) or []:\n",
    "        if ext.get(\"url\") == \"http://hl7.org/fhir/StructureDefinition/geolocation\":\n",
    "            for geo in ext.get(\"extension\", []) or []:\n",
    "                if geo.get(\"url\") == \"latitude\":\n",
    "                    latitude = geo.get(\"valueDecimal\")\n",
    "                elif geo.get(\"url\") == \"longitude\":\n",
    "                    longitude = geo.get(\"valueDecimal\")\n",
    "    return latitude, longitude\n",
    "\n",
    "\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "def insert_practitioners(practitioner_list, conn):\n",
    "    \"\"\"\n",
    "        Inserts patients, addresses, and telecoms into Postgres.\n",
    "        Expects a list of patient dicts (already parsed).\n",
    "    \"\"\"\n",
    "    insertPatients_logger = logger(\n",
    "        \"INS_patients\", \n",
    "        \"C:\\\\Users\\\\tonim\\\\Downloads\\\\output\\\\fhir\\\\ins_practitioner_log\"\n",
    "    )\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Queries\n",
    "    practitioner_query = \"\"\"\n",
    "    INSERT INTO fhir_staging.practitioners_fhir_raw (\n",
    "        practitioner_id, first_name, last_name, prefix, gender, birth_date, \n",
    "        resource\n",
    "    ) VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "    ON CONFLICT (patient_id) DO NOTHING\n",
    "    \"\"\"\n",
    "\n",
    "    address_query = \"\"\"\n",
    "    INSERT INTO fhir_staging.practioner_addresses (\n",
    "        practitioner_id, line, city, state, postal_code, country, latitude, longitude\n",
    "    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "\n",
    "    telecom_query = \"\"\"\n",
    "    INSERT INTO fhir_staging.practitioner_telecoms (\n",
    "        patient_id, system, value, use, extension\n",
    "    ) VALUES (%s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "\n",
    "for practitioner in practioner_list:\n",
    "        # Insert patient\n",
    "        cur.execute(practitioner_query, (\n",
    "            patient.get(\"patient_id\"),\n",
    "            patient.get(\"first_name\"),\n",
    "            patient.get(\"last_name\"),\n",
    "            patient.get(\"prefix\"),\n",
    "            patient.get(\"gender\"),\n",
    "            patient.get(\"birth_date\"),\n",
    "            patient.get(\"deceased_date_time\"),\n",
    "            patient.get(\"ssn\"),\n",
    "            patient.get(\"drivers_license\"),\n",
    "            patient.get(\"passport\"),\n",
    "            patient.get(\"marital_status\"),\n",
    "            patient.get(\"race\"),\n",
    "            patient.get(\"ethnicity\"),\n",
    "            patient.get(\"birth_place\"),\n",
    "            json.dumps(patient.get(\"resource\")) if patient.get(\"resource\") else None\n",
    "        ))\n",
    "\n",
    "\n",
    "\n",
    "def parse_practitioner_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "        if \"entry\" not in data:\n",
    "            return []\n",
    "\n",
    "    practitioners = []\n",
    "    for entry in data[\"entry\"]:\n",
    "        resource = entry.get(\"resource\", {})\n",
    "        if resource.get(\"resourceType\") != \"Practitioner\":\n",
    "            continue\n",
    "        id = resource.get(\"id\")\n",
    "        names = resource.get(\"name\", [])\n",
    "        name = names[0] if names else {}\n",
    "        given_name = safe_get(name.get(\"given\", [None])[0])\n",
    "        last_name = name.get(\"family\")\n",
    "        prefix = safe_get(name.get(\"prefix\", [None])[0])\n",
    "        gender = resource.get(\"gender\"),\n",
    "        extensions = safe_get(resource, \"extension\", default=[])\n",
    "        identifiers = safe_get(resource, \"identifier\", default=[])\n",
    "       \n",
    "        address = resource.get(\"address\", [])\n",
    "\n",
    "        telecom = resource.get(\"telecom\", [])\n",
    "        practitioners.append({\n",
    "            \"id\" : id,\n",
    "            \"first_name\" : given_name,\n",
    "            \"last_name\" : last_name,\n",
    "            \"prefix\" : prefix,\n",
    "            \"gender\" : gender,\n",
    "            \"extensions\" : extensions,\n",
    "            \"identifiers\" : identifiers,\n",
    "            \"address\" : address,\n",
    "            \"telecom\": telecom\n",
    "        })\n",
    "        \n",
    "    return practitioners\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    Main_logger = logger(\n",
    "        \"Main\", \n",
    "        \"C:\\\\Users\\\\tonim\\\\Downloads\\\\output\\\\fhir\\\\Main_log\"\n",
    "    )\n",
    "    #with open(\"C:\\\\Users\\\\tonim\\\\Downloads\\\\output\\\\test_patient.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    #json.dump(test_patient_data, f, indent=2)\n",
    "    \n",
    "    test_file = r\"C:\\Users\\tonim\\Downloads\\output\\fhir\\Practitioner\\practitionerInformation1755716738658.json\"\n",
    "\n",
    "    practitioner_data = [clean_nested(p) for p in (parse_practitioner_file(test_file))]\n",
    "\n",
    "    if not practitioner_data:\n",
    "        Main_logger.debug(\"No practitioner records found.\")\n",
    "    else:\n",
    "        Main_logger.debug(f\"Parsed {len(practitioner_data)} practitioners.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3573cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_encounter_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if \"entry\" not in data:\n",
    "        return []\n",
    "\n",
    "    encounters = []\n",
    "    for entry in data[\"entry\"]:\n",
    "        resource = entry.get(\"resource\", {})\n",
    "        if resource.get(\"resourceType\") != \"Encounter\":\n",
    "            continue\n",
    "\n",
    "        coding = get_first_coding(resource, \"type\")\n",
    "        encounters.append({\n",
    "            \"id\": resource.get(\"id\"),\n",
    "            \"patient_ref\": safe_get(resource, \"subject\", \"reference\"),\n",
    "            \"status\": resource.get(\"status\"),\n",
    "            \"code\": coding.get(\"code\"),\n",
    "            \"description\": coding.get(\"display\"),\n",
    "            \"start_date_time\": safe_get(resource, \"period\", \"start\"),\n",
    "            \"end_date_time\": safe_get(resource, \"period\", \"end\")\n",
    "        })\n",
    "    return encounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e201755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test code\n",
    "import json\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "test_data = {\n",
    "    \"entry\": [\n",
    "        {\n",
    "            \"resource\": {\n",
    "                \"resourceType\": \"DiagnosticReport\",\n",
    "                \"id\": \"dr-123\",\n",
    "                \"code\": {\n",
    "                    \"coding\": [\n",
    "                        {\n",
    "                            \"system\": \"http://loinc.org\",\n",
    "                            \"code\": \"12345-6\",\n",
    "                            \"display\": \"CBC Panel\"\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                \"subject\": {\"reference\": \"Patient/p-1\"},\n",
    "                \"encounter\": {\"reference\": \"Encounter/e-1\"},\n",
    "                \"effectiveDateTime\": \"2025-08-28T14:00:00Z\",\n",
    "                \"result\": [\n",
    "                    {\"reference\": \"Observation/o-1\", \"display\": \"Hemoglobin\"},\n",
    "                    {\"reference\": \"Observation/o-2\", \"display\": \"Platelet count\"}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save to file for testing\n",
    "with open(\"test_diagnostic.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(test_data, f, indent=2)\n",
    "\n",
    "# Run your function\n",
    "parsed = parse_diagnostic_reports(\"test_diagnostic.json\")\n",
    "print(json.dumps(parsed, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c948085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
