{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e8fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----Patient ETL\n",
    "import psycopg2\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "#from tqdm import tqdm\n",
    "from datetime import datetime, timezone\n",
    "#import datetime  # module\n",
    "#from datetime import datetime as dt  # class, aliased to avoid conflict\n",
    "#from datetime import datetime as tz\n",
    "#import datetime\n",
    "import json\n",
    "\n",
    "# Path to your service account JSON key\n",
    "key_path = \"/Users/toniventura/keys/bq_key.json\" \n",
    "\n",
    "# Create credentials and BigQuery client\n",
    "#credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "\n",
    "# Postgres config\n",
    "PG_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"fhir\",\n",
    "    \"user\": \"toniventura\",\n",
    "    \"password\": \"fhir_project\"\n",
    "}\n",
    "#records =[]\n",
    "\n",
    "# BigQuery config\n",
    "#BQ_PROJECT = \"your-gcp-project\"\n",
    "BQ_PROJECT = \"fhir-synthea-data\"\n",
    "BQ_DATASET = \"fhir_curated\"\n",
    "#client = bigquery.Client(project=BQ_PROJECT)\n",
    "client = bigquery.Client(project=\"fhir-synthea-data\", credentials=credentials)\n",
    "#client = bigquery.Client(project=BQ_PROJECT, credentials=credentials)\n",
    "dataset_ref = bigquery.Dataset(f\"{BQ_PROJECT}.{BQ_DATASET}\")\n",
    "\n",
    "# Helper: fetch staged data\n",
    "def fetch_staged_data(table, batch_size=10000):\n",
    "    try:\n",
    "        conn = psycopg2.connect(**PG_CONFIG)\n",
    "        cur = conn.cursor()\n",
    "        #cur.execute(f\"SELECT * FROM fhir_staging.{table}\")\n",
    "        cur.execute(\"SELECT * FROM fhir_staging.patients_fhir_raw LIMIT 5;\")\n",
    "        while True:\n",
    "            rows = cur.fetchmany(batch_size)\n",
    "            print(f\"rows: {rows}\")\n",
    "            if not rows:\n",
    "                break\n",
    "            yield rows\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Postgres connection or query failed: {e}\")\n",
    "        \n",
    "\n",
    "\n",
    "# Helper: insert dataframe into BigQuery\n",
    "def insert_to_bq(df, table_name):\n",
    "    table_id = f\"{BQ_PROJECT}.{BQ_DATASET}.{table_name}\"\n",
    "    job = client.load_table_from_dataframe(df, table_id)\n",
    "    job.result()  # wait for completion\n",
    "\n",
    "# Example: Transform & load Patients\n",
    "def transform_patients(rows):\n",
    "    #print(\"transforming data\")\n",
    "    records = []\n",
    "    print(f\"rows: {len(rows)}\")\n",
    "    for r in rows:\n",
    "        print(\"inside loop\")\n",
    "        rid, resource = r[1], r[2] # adjust index if needed\n",
    "        #print(f\"rid: {rid}\")\n",
    "        #print(f\"resource: {resource}\")\n",
    "        #birth_date = resource.get(\"birthdate\")\n",
    "        records.append({\n",
    "            \"patient_id\": rid,\n",
    "            \"first_name\": resource.get(\"name\", [{}])[0].get(\"given\", [\"\"])[0],\n",
    "            \"last_name\": resource.get(\"name\", [{}])[0].get(\"family\", \"\"),\n",
    "            #\"birth_date\": datetime.date.fromisoformat(resource.get(\"birthDate\"))\n",
    "                #if resource.get(\"birthDate\") else None,\n",
    "            \"birth_date\": datetime.fromisoformat(resource.get(\"birthDate\"))\n",
    "                if resource.get(\"birthDate\") else None,\n",
    "            \"gender\": resource.get(\"gender\"),\n",
    "            #\"load_timestamp\" : datetime.datetime.utcnow()\n",
    "            \"load_timestamp\": datetime.now(timezone.utc)\n",
    "        })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Main ETL loop\n",
    "def etl_patients():\n",
    "    try:\n",
    "        for batch in tqdm(fetch_staged_data(\"patients_fhir_raw\")):\n",
    "            df = transform_patients(batch)\n",
    "            if not df.empty:\n",
    "                insert_to_bq(df, \"patients\")\n",
    "                print(\"***Inserting***\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in patients ETL: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    etl_patients()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aacb4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'system': 'http://hl7.org/fhir/sid/us-npi', 'value': '9999928192'}\n",
      "system: http://hl7.org/fhir/sid/us-npi\n",
      "value: 9999928192\n",
      "---- Results ----\n",
      "NPI: 9999928192\n",
      "License Number: None\n",
      "Other IDs: []\n",
      "***Inserted batch***\n"
     ]
    }
   ],
   "source": [
    "#---Practitioner ETL\n",
    "import psycopg2\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "# Path to your service account JSON key\n",
    "key_path = \"/Users/toniventura/keys/bq_key.json\"\n",
    "\n",
    "# Create credentials and BigQuery client\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "BQ_PROJECT = \"fhir-synthea-data\"\n",
    "BQ_DATASET = \"fhir_curated\"\n",
    "client = bigquery.Client(project=BQ_PROJECT, credentials=credentials)\n",
    "\n",
    "# Postgres config\n",
    "PG_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"fhir\",\n",
    "    \"user\": \"toniventura\",\n",
    "    \"password\": \"fhir_project\"\n",
    "}\n",
    "\n",
    "# Helper: fetch staged data\n",
    "def fetch_staged_data(table, batch_size=10000):\n",
    "    try:\n",
    "        conn = psycopg2.connect(**PG_CONFIG)\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(f\"SELECT * FROM fhir_staging.{table} LIMIT 5;\")\n",
    "        while True:\n",
    "            rows = cur.fetchmany(batch_size)\n",
    "            if not rows:\n",
    "                break\n",
    "            yield rows\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Postgres connection or query failed: {e}\")\n",
    "\n",
    "# Helper: insert dataframe into BigQuery\n",
    "def insert_to_bq(df, table_name):\n",
    "    table_id = f\"{BQ_PROJECT}.{BQ_DATASET}.{table_name}\"\n",
    "    job = client.load_table_from_dataframe(df, table_id)\n",
    "    job.result()\n",
    "\n",
    "# Transform Practitioners\n",
    "def transform_practitioners(rows):\n",
    "    records = []\n",
    "    for r in rows:\n",
    "        rid, resource = r[1], r[2]  # adjust index if needed\n",
    "\n",
    "        #Initialize variables so they exist even if not found\n",
    "        npi = None\n",
    "        license_number = None\n",
    "        other_ids = []\n",
    "\n",
    "    for ident in resource.get(\"identifier\", []):\n",
    "        # Debugging: show the whole identifier object\n",
    "        pprint.pprint(ident)\n",
    "\n",
    "        system = ident.get(\"system\")\n",
    "        value = ident.get(\"value\")\n",
    "\n",
    "        print(f\"system: {system}\")\n",
    "        print(f\"value: {value}\")\n",
    "\n",
    "        if system == \"http://hl7.org/fhir/sid/us-npi\":\n",
    "            npi = value\n",
    "        elif system == \"http://example.org/license-number\":\n",
    "            license_number = value\n",
    "        else:\n",
    "            other_ids.append(value)\n",
    "\n",
    "\n",
    "        print(\"---- Results ----\")\n",
    "        print(f\"NPI: {npi}\")\n",
    "        print(f\"License Number: {license_number}\")\n",
    "        print(f\"Other IDs: {other_ids}\")\n",
    "        name_info = resource.get(\"name\", [{}])[0]\n",
    "\n",
    "        records.append({\n",
    "            \"practitioner_id\": rid,\n",
    "            \"first_name\": name_info.get(\"given\", [\"\"])[0],\n",
    "            \"last_name\": name_info.get(\"family\", \"\"),\n",
    "            \"prefix\": name_info.get(\"prefix\", [\"\"])[0] if name_info.get(\"prefix\") else None,\n",
    "            \"gender\": resource.get(\"gender\"),\n",
    "            #\"birth_date\": datetime.fromisoformat(resource.get(\"birthDate\")) if resource.get(\"birthDate\") else None,\n",
    "            \"npi\": npi,\n",
    "            \"license_number\": license_number,\n",
    "            \"primary_email\": next((t.get(\"value\") for t in resource.get(\"telecom\", []) if t.get(\"system\") == \"email\"), None),\n",
    "            \"primary_phone\": next((t.get(\"value\") for t in resource.get(\"telecom\", []) if t.get(\"system\") == \"phone\"), None),\n",
    "            \"load_timestamp\": datetime.now(timezone.utc)\n",
    "        })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Main ETL loop\n",
    "def etl_practitioners():\n",
    "    try:\n",
    "        for batch in fetch_staged_data(\"practitioners_fhir_raw\"):\n",
    "            df = transform_practitioners(batch)\n",
    "            if not df.empty:\n",
    "                insert_to_bq(df, \"practitioners\")\n",
    "                print(\"***Inserted batch***\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in practitioners ETL: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    etl_practitioners()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ad0b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208D00000X\n",
      "General Practice Physician\n",
      "General Practice Physician\n",
      "208D00000X\n",
      "208D00000X\n",
      "General Practice Physician\n",
      "General Practice Physician\n",
      "208D00000X\n",
      "208D00000X\n",
      "General Practice Physician\n",
      "General Practice Physician\n",
      "208D00000X\n",
      "208D00000X\n",
      "General Practice Physician\n",
      "General Practice Physician\n",
      "208D00000X\n",
      "208D00000X\n",
      "General Practice Physician\n",
      "General Practice Physician\n",
      "208D00000X\n",
      "***Inserted batch***\n"
     ]
    }
   ],
   "source": [
    "#---Practitioner Roles ETL\n",
    "import psycopg2\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "# Path to your service account JSON key\n",
    "key_path = \"/Users/toniventura/keys/bq_key.json\"\n",
    "\n",
    "# Create credentials and BigQuery client\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "BQ_PROJECT = \"fhir-synthea-data\"\n",
    "BQ_DATASET = \"fhir_curated\"\n",
    "client = bigquery.Client(project=BQ_PROJECT, credentials=credentials)\n",
    "\n",
    "# Postgres config\n",
    "PG_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"fhir\",\n",
    "    \"user\": \"toniventura\",\n",
    "    \"password\": \"fhir_project\"\n",
    "}\n",
    "\n",
    "def insert_to_bq(df, table_name):\n",
    "    table_id = f\"{BQ_PROJECT}.{BQ_DATASET}.{table_name}\"\n",
    "    job = client.load_table_from_dataframe(df, table_id)\n",
    "    job.result()\n",
    "\n",
    "def fetch_staged_data(table, batch_size=10000):\n",
    "    try:\n",
    "        conn = psycopg2.connect(**PG_CONFIG)\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(f\"SELECT * FROM fhir_staging.{table} LIMIT 5;\")\n",
    "        while True:\n",
    "            rows = cur.fetchmany(batch_size)\n",
    "            if not rows:\n",
    "                break\n",
    "            yield rows\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Postgres connection or query failed: {e}\")\n",
    "\n",
    "# Helper: insert dataframe into BigQuery\n",
    "def insert_to_bq(df, table_name):\n",
    "    table_id = f\"{BQ_PROJECT}.{BQ_DATASET}.{table_name}\"\n",
    "    job = client.load_table_from_dataframe(df, table_id)\n",
    "    job.result()\n",
    "\n",
    "# Transform Practitioners\n",
    "def transform_practitioner_roles(rows):\n",
    "    records = []\n",
    "    for r in rows:\n",
    "        rid, resource = r[1], r[2]\n",
    "\n",
    "        # Initialize columns\n",
    "        specialty_text = resource.get(\"specialty\", {})[0].get(\"text\")\n",
    "        #specialty_code = resource.get(\"specialty\", {})[0].get(\"code\",[{}][0].get(\"coding\",[{}][0].get(\"code\")))\n",
    "        specialty_code = resource.get(\"specialty\", [{}])[0].get(\"coding\", [{}])[0].get(\"code\")\n",
    "        role_text = resource.get(\"code\", [{}])[0].get(\"text\")\n",
    "        role_code = resource.get(\"code\", [{}])[0].get(\"coding\", [{}])[0].get(\"code\")\n",
    "        \n",
    "        print(specialty_code)\n",
    "        print(specialty_text)\n",
    "        print(role_text)\n",
    "        print(role_code)\n",
    "\n",
    "        \n",
    "\n",
    "        records.append({\n",
    "            \"practitioner_role_id\": rid,\n",
    "            \"practitioner_npi\": resource.get(\"practitioner\", {}).get(\"identifier\").get(\"value\"),\n",
    "            \"organization_id\": resource.get(\"organization\", {}).get(\"identifier\").get(\"value\"),\n",
    "            \"specialty_code\": specialty_code,\n",
    "            \"specialty_text\": specialty_text,\n",
    "            \"role_text\" : role_text, \n",
    "            \"role_code\": role_code, \n",
    "            #                 if resource.get(\"telecom\") and resource[\"telecom\"][0].get(\"system\")==\"email\" else None,\n",
    "            #\"role_text\": resource.get(\"telecom\", [{}])[0].get(\"value\") \n",
    "                              #if resource.get(\"telecom\") and resource[\"telecom\"][0].get(\"system\")==\"phone\" else None,\n",
    "            \"load_timestamp\": datetime.now(timezone.utc)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "# Main ETL loop\n",
    "def etl_practitioner_roles():\n",
    "    try:\n",
    "        for batch in fetch_staged_data(\"practitioner_roles_fhir_raw\"):\n",
    "            df = transform_practitioner_roles(batch)\n",
    "            if not df.empty:\n",
    "                insert_to_bq(df, \"practitioner_roles\")\n",
    "                print(\"***Inserted batch***\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in practitioner roles ETL: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    etl_practitioner_roles()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75baae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 rows to fhir-synthea-data.fhir_curated.observations\n",
      "***Inserted batch***\n"
     ]
    }
   ],
   "source": [
    "#---Observations ETL\n",
    "import psycopg2\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import pprint\n",
    "import dateutil.parser  # optional, for robust ISO parsing\n",
    "\n",
    "# Path to your service account JSON key\n",
    "key_path = \"/Users/toniventura/keys/bq_key.json\"\n",
    "\n",
    "# Create credentials and BigQuery client\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "BQ_PROJECT = \"fhir-synthea-data\"\n",
    "BQ_DATASET = \"fhir_curated\"\n",
    "client = bigquery.Client(project=BQ_PROJECT, credentials=credentials)\n",
    "\n",
    "# Postgres config\n",
    "PG_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"fhir\",\n",
    "    \"user\": \"toniventura\",\n",
    "    \"password\": \"fhir_project\"\n",
    "}\n",
    "\n",
    "def insert_to_bq(df, table_name):\n",
    "    table_id = f\"{BQ_PROJECT}.{BQ_DATASET}.{table_name}\"\n",
    "    job = client.load_table_from_dataframe(df, table_id)\n",
    "    job.result()\n",
    "    print(f\"Loaded {job.output_rows} rows to {table_id}\")\n",
    "\n",
    "\n",
    "\n",
    "# Helper: fetch staged data\n",
    "def fetch_staged_data(table, batch_size=10000):\n",
    "    try:\n",
    "        conn = psycopg2.connect(**PG_CONFIG)\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(f\"SELECT * FROM fhir_staging.{table} LIMIT 5;\")\n",
    "        while True:\n",
    "            rows = cur.fetchmany(batch_size)\n",
    "            if not rows:\n",
    "                break\n",
    "            yield rows\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Postgres connection or query failed: {e}\")\n",
    "\n",
    "# Transform Practitioners\n",
    "def transform_observations(rows):\n",
    "    records = []\n",
    "    for r in rows:\n",
    "        rid, resource = r[1], r[2]\n",
    "\n",
    "        # Initialize columns\n",
    "        codings  = resource.get(\"code\", {}).get(\"coding\", [])\n",
    "        code_text = resource.get(\"code\", {}).get(\"text\")\n",
    "\n",
    "        status = resource.get(\"status\") if codings else None\n",
    "        system = codings[0].get(\"system\")if codings else None\n",
    "        code = codings[0].get(\"code\") if codings else None\n",
    "        \n",
    "        codings_struct = [\n",
    "            {\n",
    "                \"system\" : c.get(\"system\"), \n",
    "                \"code\" : c.get(\"code\"), \n",
    "                \"display\": c.get(\"display\")\n",
    "                \n",
    "            }for c in codings\n",
    "        ]\n",
    "\n",
    "        value_numeric = None\n",
    "        unit = None\n",
    "        value_text = None\n",
    "        value_codings = []\n",
    "\n",
    "        if \"valueQuantity\" in resource:\n",
    "            q = resource[\"valueQuantity\"]\n",
    "            value_numeric = q.get(\"value\")\n",
    "            unit = q.get(\"unit\")\n",
    "            # system/code available but usually redundant here\n",
    "\n",
    "        elif \"valueString\" in resource:\n",
    "            value_text = resource[\"valueString\"]\n",
    "\n",
    "        elif \"valueCodeableConcept\" in resource:\n",
    "            cc = resource[\"valueCodeableConcept\"]\n",
    "            # Store text in value_text for quick querying\n",
    "            value_text = cc.get(\"text\")\n",
    "            # Capture codings for full fidelity\n",
    "            for c in cc.get(\"coding\", []):\n",
    "                value_codings.append({\n",
    "                \"system\": c.get(\"system\"),\n",
    "                \"code\": c.get(\"code\"),\n",
    "                \"display\": c.get(\"display\")\n",
    "             })\n",
    "\n",
    "        elif \"valueDateTime\" in resource:\n",
    "            value_text = resource[\"valueDateTime\"]  # or a dedicated column\n",
    "\n",
    "        elif \"valuePeriod\" in resource:\n",
    "            value_text = json.dumps(resource[\"valuePeriod\"])  # or expand into start/end columns\n",
    "\n",
    "        patient_id_ref = resource.get(\"subject\", {}).get(\"reference\")\n",
    "        patient_id = patient_id_ref.split(\":\")[-1]\n",
    "\n",
    "        encounter_id_ref = resource.get(\"encounter\",{}).get(\"reference\")\n",
    "        encounter_id = encounter_id_ref.split(\":\")[-1]\n",
    "\n",
    "        effective_date_str = resource.get(\"effectiveDateTime\")\n",
    "        effective_date = None\n",
    "\n",
    "        if effective_date_str:\n",
    "            effective_datetime = dateutil.parser.isoparse(effective_date_str)\n",
    "        \n",
    "        records.append({\n",
    "            \"observation_id\": rid,\n",
    "            \"status\" : status,\n",
    "            \"obs_code\": code,\n",
    "            \"system\": system,\n",
    "            \"obs_code_text\": code_text,\n",
    "            \"codings\": codings_struct,\n",
    "            \"value_numeric\": value_numeric, \n",
    "            \"value_text\" : value_text,\n",
    "            \"unit\" : unit,\n",
    "            \"value_codings\" : value_codings, \n",
    "            \"patient_id\" : patient_id,\n",
    "            \"encounter_id\" : encounter_id,\n",
    "            \"effective_datetime\": effective_date, #not inserting\n",
    "            \"load_timestamp\": datetime.now(timezone.utc) \n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "# Main ETL loop\n",
    "def etl_observations():\n",
    "    try:\n",
    "        for batch in fetch_staged_data(\"observations_fhir_raw\"):\n",
    "            df = transform_observations(batch)\n",
    "            if not df.empty:\n",
    "                insert_to_bq(df, \"observations\")\n",
    "                print(\"***Inserted batch***\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in observations ETL: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    etl_observations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a69ce39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 rows to fhir-synthea-data.fhir_curated.conditions\n",
      "***Inserted batch***\n"
     ]
    }
   ],
   "source": [
    "#---Conditions ETL\n",
    "import psycopg2\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import pprint\n",
    "import dateutil.parser  # optional, for robust ISO parsing\n",
    "\n",
    "# Path to your service account JSON key\n",
    "key_path = \"/Users/toniventura/keys/bq_key.json\"\n",
    "\n",
    "# Create credentials and BigQuery client\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "BQ_PROJECT = \"fhir-synthea-data\"\n",
    "BQ_DATASET = \"fhir_curated\"\n",
    "client = bigquery.Client(project=BQ_PROJECT, credentials=credentials)\n",
    "\n",
    "# Postgres config\n",
    "PG_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"fhir\",\n",
    "    \"user\": \"toniventura\",\n",
    "    \"password\": \"fhir_project\"\n",
    "}\n",
    "\n",
    "def insert_to_bq(df, table_name):\n",
    "    table_id = f\"{BQ_PROJECT}.{BQ_DATASET}.{table_name}\"\n",
    "    job = client.load_table_from_dataframe(df, table_id)\n",
    "    job.result()\n",
    "    print(f\"Loaded {job.output_rows} rows to {table_id}\")\n",
    "\n",
    "\n",
    "\n",
    "# Helper: fetch staged data\n",
    "def fetch_staged_data(table, batch_size=10000):\n",
    "    try:\n",
    "        conn = psycopg2.connect(**PG_CONFIG)\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(f\"SELECT * FROM fhir_staging.{table} LIMIT 5;\")\n",
    "        while True:\n",
    "            rows = cur.fetchmany(batch_size)\n",
    "            if not rows:\n",
    "                break\n",
    "            yield rows\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Postgres connection or query failed: {e}\")\n",
    "\n",
    "# Transform Practitioners\n",
    "def transform_conditions(rows):\n",
    "    records = []\n",
    "    for r in rows:\n",
    "        rid, resource = r[1], r[2]\n",
    "\n",
    "        # Initialize columns\n",
    "        codings  = resource.get(\"code\", {}).get(\"coding\", [])\n",
    "        code_text = resource.get(\"code\", {}).get(\"text\")\n",
    "\n",
    "        status = resource.get(\"status\") if codings else None\n",
    "        system = codings[0].get(\"system\")if codings else None\n",
    "        code = codings[0].get(\"code\") if codings else None\n",
    "        \n",
    "        codings_struct = [\n",
    "            {\n",
    "                \"system\" : c.get(\"system\"), \n",
    "                \"code\" : c.get(\"code\"), \n",
    "                \"display\": c.get(\"display\")\n",
    "                \n",
    "            }for c in codings\n",
    "        ]\n",
    "\n",
    "        category = None\n",
    "        category_text = None\n",
    "\n",
    "        if resource.get(\"category\"):\n",
    "            first_category = resource[\"category\"][0][\"coding\"][0]  # first category -> first coding\n",
    "            category = first_category.get(\"code\")\n",
    "            category_text = first_category.get(\"display\")\n",
    "        else:\n",
    "            category = None\n",
    "            category_text = None\n",
    "\n",
    "        # Nested array for full fidelity\n",
    "        category_codings = []\n",
    "        for cat in resource.get(\"category\", []):\n",
    "            for coding in cat.get(\"coding\", []):\n",
    "                category_codings.append({\n",
    "                \"system\": coding.get(\"system\"),\n",
    "                \"code\": coding.get(\"code\"),\n",
    "                \"display\": coding.get(\"display\")\n",
    "             })\n",
    "\n",
    "    \n",
    "        patient_id_ref = resource.get(\"subject\", {}).get(\"reference\")\n",
    "        patient_id = patient_id_ref.split(\":\")[-1]\n",
    "\n",
    "        encounter_id_ref = resource.get(\"encounter\",{}).get(\"reference\")\n",
    "        encounter_id = encounter_id_ref.split(\":\")[-1]\n",
    "\n",
    "        onset_date_str = resource.get(\"effectiveDateTime\")\n",
    "        onset_date_time = None\n",
    "\n",
    "        if onset_date_str:\n",
    "            onset_date_time = dateutil.parser.isoparse(onset_date_str)\n",
    "        \n",
    "        records.append({\n",
    "            \"condition_id\": rid,\n",
    "            \"clinical_status\" : status,\n",
    "            \"code\": code,\n",
    "            \"code_system\": system,\n",
    "            \"code_text\": code_text,\n",
    "            \"codings\": codings_struct,\n",
    "            \"category_code\": category,\n",
    "            \"category\": category_text,\n",
    "            \"category_codings\": category_codings,\n",
    "            \"patient_id\" : patient_id,\n",
    "            \"encounter_id\" : encounter_id,\n",
    "            \"onset_date\": onset_date_time, #not inserting\n",
    "            \"load_timestamp\": datetime.now(timezone.utc) \n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "# Main ETL loop\n",
    "def etl_conditions():\n",
    "    try:\n",
    "        for batch in fetch_staged_data(\"conditions_fhir_raw\"):\n",
    "            df = transform_conditions(batch)\n",
    "            if not df.empty:\n",
    "                insert_to_bq(df, \"conditions\")\n",
    "                print(\"***Inserted batch***\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in conditions ETL: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    etl_conditions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb9efc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use: claim\n",
      "status: active\n",
      "patient_id: 9cfbd2b4-5eff-91db-1a93-777a9b86738f\n",
      "type_info: None\n",
      "Error in claims ETL: 'NoneType' object has no attribute 'get'\n"
     ]
    }
   ],
   "source": [
    "#---Claims ETL\n",
    "import psycopg2\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import pprint\n",
    "import dateutil.parser  # optional, for robust ISO parsing\n",
    "\n",
    "# Path to your service account JSON key\n",
    "key_path = \"/Users/toniventura/keys/bq_key.json\"\n",
    "\n",
    "# Create credentials and BigQuery client\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "BQ_PROJECT = \"fhir-synthea-data\"\n",
    "BQ_DATASET = \"fhir_curated\"\n",
    "client = bigquery.Client(project=BQ_PROJECT, credentials=credentials)\n",
    "\n",
    "# Postgres config\n",
    "PG_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"fhir\",\n",
    "    \"user\": \"toniventura\",\n",
    "    \"password\": \"fhir_project\"\n",
    "}\n",
    "\n",
    "def insert_to_bq(df, table_name):\n",
    "    table_id = f\"{BQ_PROJECT}.{BQ_DATASET}.{table_name}\"\n",
    "    job = client.load_table_from_dataframe(df, table_id)\n",
    "    job.result()\n",
    "    print(f\"Loaded {job.output_rows} rows to {table_id}\")\n",
    "\n",
    "\n",
    "\n",
    "# Helper: fetch staged data\n",
    "def fetch_staged_data(table, batch_size=10000):\n",
    "    try:\n",
    "        conn = psycopg2.connect(**PG_CONFIG)\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(f\"SELECT * FROM fhir_staging.{table} LIMIT 5;\")\n",
    "        while True:\n",
    "            rows = cur.fetchmany(batch_size)\n",
    "            if not rows:\n",
    "                break\n",
    "            yield rows\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Postgres connection or query failed: {e}\")\n",
    "\n",
    "# Transform Practitioners\n",
    "def transform_claims(rows):\n",
    "    records = []\n",
    "    for r in rows:\n",
    "        rid, resource = r[1], r[2]\n",
    "\n",
    "        use = resource.get(\"use\")\n",
    "        print(f\"use: {use}\")\n",
    "\n",
    "        status = resource.get(\"status\")\n",
    "        print(f\"status: {status}\")\n",
    "\n",
    "        patient_id = resource.get(\"patient\", {}).get(\"reference\").split(\":\")[-1]\n",
    "        print(f\"patient_id: {patient_id}\")\n",
    "\n",
    "        type_info = resource.get(\"type\", {}).get(\"code\")\n",
    "        type_info = type_info[0] if type_info else None\n",
    "\n",
    "        print(f\"type_info: {type_info}\")\n",
    "\n",
    "        total_value = resource.get(\"total\",{}).get(\"value\")\n",
    "        total_currency = resource.get(\"total\", {}).get(\"currency\")\n",
    "\n",
    "        billable_start = resource.get(\"billable_period\", {}).get(\"start\")\n",
    "        billable_end = resource.get(\"billable_period\", {}).get(\"end\")\n",
    "\n",
    "        date_created = resource.get(\"created\")\n",
    "\n",
    "        billing_provider = []\n",
    "        provider_reference = resource.get(\"provider\", {}).get(\"reference\")\n",
    "        provider_reference_id = provider_reference.split(\"|\")[-1]\n",
    "        provider_reference_type = provider_reference.split(\"?\")[0]\n",
    "        provider_display = resource.get(\"provider\", {}).get(\"display\")\n",
    "        billing_provider.append({\n",
    "            \"provider_reference_id\": provider_reference_id,\n",
    "            \"provider_reference_type\": provider_reference_type,\n",
    "            \"provider_display\": provider_display \n",
    "        })\n",
    "\n",
    "        priority_code = (resource.get(\"priority\", {}).get(\"code\") or [None])[0]\n",
    "\n",
    "        facility_json = resource.get(\"facility\")\n",
    "        facility = []\n",
    "        facility_reference_complete = facility_json.get(\"reference\")\n",
    "        facility_reference = facility_reference_complete.split(\"|\")[0]\n",
    "        facility_id = facility_reference_complete.split(\"|\")[-1]\n",
    "    \n",
    "        facility.append({\n",
    "            \"facility_reference\": facility_reference,\n",
    "            #\"facility_reference\": facility_json.get(\"reference\"),\n",
    "            \"facility_id\": facility_id,\n",
    "            \"facility_display\" : facility_json.get(\"display\")   \n",
    "        })\n",
    "\n",
    "        insurances = resource.get(\"insurance\",[{}])\n",
    "        all_insurances = []\n",
    "\n",
    "        for insurance in insurances:\n",
    "            print(insurance.get(\"sequence\"))\n",
    "            print(insurance.get(\"focal\"))\n",
    "            print(insurance.get(\"coverage\"))\n",
    "            all_insurances.append({\n",
    "                \"sequence\": insurance.get(\"sequence\"),\n",
    "                \"focal\": insurance.get(\"focal\"),\n",
    "                \"coverage\": insurance.get(\"coverage\", {}).get(\"display\")\n",
    "            })\n",
    "        \n",
    "        \n",
    "        items = resource.get(\"items\", [{}])\n",
    "        all_items = []\n",
    "\n",
    "        for item in items:\n",
    "            sequence = \"None\"\n",
    "            sequence = item.get(\"sequence\")\n",
    "            item_type = \"None\"\n",
    "            diagnosis_seq = item.get(\"diagnosisSequence\")\n",
    "            information_seq = item.get(\"informationSequence\")\n",
    "            procedure_seq = item.get(\"procedureSequence\")\n",
    "            productOrService = item.get(\"productOrService\")\n",
    "            system = \"None\"\n",
    "            code = \"None\"\n",
    "            display = \"None\"\n",
    "            coding = productOrService.get(\"coding\",[0])\n",
    "            system = coding.get(\"system\")\n",
    "            code = coding.get(\"code\")\n",
    "            display = coding.get(\"display\")\n",
    "            service_period = item.get(\"servicePeriod\",{})\n",
    "            start = service_period.get(\"start\")\n",
    "            end = service_period.get(\"end\")\n",
    "\n",
    "            if diagnosis_seq:\n",
    "                item_type = \"diagnosis sequence\"  \n",
    "            if information_seq:\n",
    "                item_type = \"information sequence\"\n",
    "            if procedure_seq:\n",
    "                item_type = \"procedure sequence\"  \n",
    "\n",
    "            location = []\n",
    "\n",
    "            location_coding = item.get(\"locationCodeableConcept\").get(\"coding\",[0])\n",
    "            location_system = location_coding.get(\"system\")\n",
    "            location_code = location_coding.get(\"code\")\n",
    "            location_display = location_coding.get(\"display\")\n",
    "\n",
    "            location.append({\n",
    "                \"facility_id\": facility_id,\n",
    "                \"system\": location_system,\n",
    "                \"code\": location_code,\n",
    "                \"display\": location_display\n",
    "            })\n",
    "\n",
    "            encounter = item.get(\"encounter\",[0]).get(\"reference\").split(\":\")[-1]\n",
    "\n",
    "            net = item.get(\"net\",{})\n",
    "            net_value = net.get(\"value\")\n",
    "            net_currency = net.get(\"currency\")\n",
    "\n",
    "            item_text =item.get(\"text\")\n",
    "\n",
    "            all_items.append({\n",
    "                \"sequence\": sequence,\n",
    "                \"item_type\": item_type,\n",
    "                \"system\": system,\n",
    "                \"code\": code,\n",
    "                \"display\": display,\n",
    "                \"service_start\":start,\n",
    "                \"service_end\": end,\n",
    "                \"net_value\": net_value,\n",
    "                \"net_currency\": net_currency,\n",
    "                \"location\": location,\n",
    "                \"encounter\": encounter,\n",
    "                \"start_period\": start,\n",
    "                \"end_period\": end,\n",
    "                \"item_text\": item_text\n",
    "            })\n",
    "\n",
    "        # Initialize columns\n",
    "        all_diagnosis = []\n",
    "        diagnoses = resource.get(\"diagnosis\",[])\n",
    "\n",
    "        for diagnosis in diagnoses:\n",
    "            sequence = diagnosis.get(\"sequence\")\n",
    "            diagnosis_id = diagnosis.get(\"diagnosisReference\").get(\"reference\").split(\":\")[-1] #conditions\n",
    "            all_diagnosis.append({\n",
    "                \"sequence\": sequence,\n",
    "                \"diagnosis\": \"diagnosis_id\"    \n",
    "            })\n",
    "\n",
    "        records.append({\n",
    "            \"claim_id\": rid,\n",
    "            \"status\" : status,\n",
    "            \"use\": use,\n",
    "            \"status\": status,\n",
    "            \"patient_id\": patient_id,\n",
    "            \"claim_type_info\": type_info,\n",
    "            \"total_value\": total_value,\n",
    "            \"total_currency\": total_currency,\n",
    "            \"billable_start\": billable_start,\n",
    "            \"billable_end\" : billable_end,\n",
    "            \"created\": date_created,\n",
    "            \"billing_provider\" : billing_provider,\n",
    "            \"priority_code\": priority_code,\n",
    "            \"date_created\": date_created,\n",
    "            \"facility\" : facility,\n",
    "            \"all_insurances\": all_insurances,\n",
    "            \"diagnoses\": all_diagnosis,\n",
    "            \"items\":all_items,\n",
    "            \"load_timestamp\": datetime.now(timezone.utc) \n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "# Main ETL loop\n",
    "def etl_claims():\n",
    "    try:\n",
    "        for batch in fetch_staged_data(\"claims_fhir_raw\"):\n",
    "            df = transform_claims(batch)\n",
    "            if not df.empty:\n",
    "                insert_to_bq(df, \"claims\")\n",
    "                print(\"***Inserted batch***\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in claims ETL: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    etl_claims()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
