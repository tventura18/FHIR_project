{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e8fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----Patient ETL\n",
    "import psycopg2\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "#from tqdm import tqdm\n",
    "from datetime import datetime, timezone\n",
    "#import datetime  # module\n",
    "#from datetime import datetime as dt  # class, aliased to avoid conflict\n",
    "#from datetime import datetime as tz\n",
    "#import datetime\n",
    "import json\n",
    "\n",
    "# Path to your service account JSON key\n",
    "key_path = \"/Users/toniventura/keys/bq_key.json\" \n",
    "\n",
    "# Create credentials and BigQuery client\n",
    "#credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "\n",
    "# Postgres config\n",
    "PG_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"fhir\",\n",
    "    \"user\": \"toniventura\",\n",
    "    \"password\": \"fhir_project\"\n",
    "}\n",
    "#records =[]\n",
    "\n",
    "# BigQuery config\n",
    "#BQ_PROJECT = \"your-gcp-project\"\n",
    "BQ_PROJECT = \"fhir-synthea-data\"\n",
    "BQ_DATASET = \"fhir_curated\"\n",
    "#client = bigquery.Client(project=BQ_PROJECT)\n",
    "client = bigquery.Client(project=\"fhir-synthea-data\", credentials=credentials)\n",
    "#client = bigquery.Client(project=BQ_PROJECT, credentials=credentials)\n",
    "dataset_ref = bigquery.Dataset(f\"{BQ_PROJECT}.{BQ_DATASET}\")\n",
    "\n",
    "# Helper: fetch staged data\n",
    "def fetch_staged_data(table, batch_size=10000):\n",
    "    try:\n",
    "        conn = psycopg2.connect(**PG_CONFIG)\n",
    "        cur = conn.cursor()\n",
    "        #cur.execute(f\"SELECT * FROM fhir_staging.{table}\")\n",
    "        cur.execute(\"SELECT * FROM fhir_staging.patients_fhir_raw LIMIT 5;\")\n",
    "        while True:\n",
    "            rows = cur.fetchmany(batch_size)\n",
    "            print(f\"rows: {rows}\")\n",
    "            if not rows:\n",
    "                break\n",
    "            yield rows\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Postgres connection or query failed: {e}\")\n",
    "        \n",
    "\n",
    "\n",
    "# Helper: insert dataframe into BigQuery\n",
    "def insert_to_bq(df, table_name):\n",
    "    table_id = f\"{BQ_PROJECT}.{BQ_DATASET}.{table_name}\"\n",
    "    job = client.load_table_from_dataframe(df, table_id)\n",
    "    job.result()  # wait for completion\n",
    "\n",
    "# Example: Transform & load Patients\n",
    "def transform_patients(rows):\n",
    "    #print(\"transforming data\")\n",
    "    records = []\n",
    "    print(f\"rows: {len(rows)}\")\n",
    "    for r in rows:\n",
    "        print(\"inside loop\")\n",
    "        rid, resource = r[1], r[2] # adjust index if needed\n",
    "        #print(f\"rid: {rid}\")\n",
    "        #print(f\"resource: {resource}\")\n",
    "        #birth_date = resource.get(\"birthdate\")\n",
    "        records.append({\n",
    "            \"patient_id\": rid,\n",
    "            \"first_name\": resource.get(\"name\", [{}])[0].get(\"given\", [\"\"])[0],\n",
    "            \"last_name\": resource.get(\"name\", [{}])[0].get(\"family\", \"\"),\n",
    "            #\"birth_date\": datetime.date.fromisoformat(resource.get(\"birthDate\"))\n",
    "                #if resource.get(\"birthDate\") else None,\n",
    "            \"birth_date\": datetime.fromisoformat(resource.get(\"birthDate\"))\n",
    "                if resource.get(\"birthDate\") else None,\n",
    "            \"gender\": resource.get(\"gender\"),\n",
    "            #\"load_timestamp\" : datetime.datetime.utcnow()\n",
    "            \"load_timestamp\": datetime.now(timezone.utc)\n",
    "        })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Main ETL loop\n",
    "def etl_patients():\n",
    "    try:\n",
    "        for batch in tqdm(fetch_staged_data(\"patients_fhir_raw\")):\n",
    "            df = transform_patients(batch)\n",
    "            if not df.empty:\n",
    "                insert_to_bq(df, \"patients\")\n",
    "                print(\"***Inserting***\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in patients ETL: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    etl_patients()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aacb4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'system': 'http://hl7.org/fhir/sid/us-npi', 'value': '9999928192'}\n",
      "system: http://hl7.org/fhir/sid/us-npi\n",
      "value: 9999928192\n",
      "---- Results ----\n",
      "NPI: 9999928192\n",
      "License Number: None\n",
      "Other IDs: []\n",
      "***Inserted batch***\n"
     ]
    }
   ],
   "source": [
    "#---Practitioner ETL\n",
    "import psycopg2\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "# Path to your service account JSON key\n",
    "key_path = \"/Users/toniventura/keys/bq_key.json\"\n",
    "\n",
    "# Create credentials and BigQuery client\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "BQ_PROJECT = \"fhir-synthea-data\"\n",
    "BQ_DATASET = \"fhir_curated\"\n",
    "client = bigquery.Client(project=BQ_PROJECT, credentials=credentials)\n",
    "\n",
    "# Postgres config\n",
    "PG_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"fhir\",\n",
    "    \"user\": \"toniventura\",\n",
    "    \"password\": \"fhir_project\"\n",
    "}\n",
    "\n",
    "# Helper: fetch staged data\n",
    "def fetch_staged_data(table, batch_size=10000):\n",
    "    try:\n",
    "        conn = psycopg2.connect(**PG_CONFIG)\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(f\"SELECT * FROM fhir_staging.{table} LIMIT 5;\")\n",
    "        while True:\n",
    "            rows = cur.fetchmany(batch_size)\n",
    "            if not rows:\n",
    "                break\n",
    "            yield rows\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Postgres connection or query failed: {e}\")\n",
    "\n",
    "# Helper: insert dataframe into BigQuery\n",
    "def insert_to_bq(df, table_name):\n",
    "    table_id = f\"{BQ_PROJECT}.{BQ_DATASET}.{table_name}\"\n",
    "    job = client.load_table_from_dataframe(df, table_id)\n",
    "    job.result()\n",
    "\n",
    "# Transform Practitioners\n",
    "def transform_practitioners(rows):\n",
    "    records = []\n",
    "    for r in rows:\n",
    "        rid, resource = r[1], r[2]  # adjust index if needed\n",
    "\n",
    "        #Initialize variables so they exist even if not found\n",
    "        npi = None\n",
    "        license_number = None\n",
    "        other_ids = []\n",
    "\n",
    "    for ident in resource.get(\"identifier\", []):\n",
    "        # Debugging: show the whole identifier object\n",
    "        pprint.pprint(ident)\n",
    "\n",
    "        system = ident.get(\"system\")\n",
    "        value = ident.get(\"value\")\n",
    "\n",
    "        print(f\"system: {system}\")\n",
    "        print(f\"value: {value}\")\n",
    "\n",
    "        if system == \"http://hl7.org/fhir/sid/us-npi\":\n",
    "            npi = value\n",
    "        elif system == \"http://example.org/license-number\":\n",
    "            license_number = value\n",
    "        else:\n",
    "            other_ids.append(value)\n",
    "\n",
    "\n",
    "        print(\"---- Results ----\")\n",
    "        print(f\"NPI: {npi}\")\n",
    "        print(f\"License Number: {license_number}\")\n",
    "        print(f\"Other IDs: {other_ids}\")\n",
    "        name_info = resource.get(\"name\", [{}])[0]\n",
    "\n",
    "        records.append({\n",
    "            \"practitioner_id\": rid,\n",
    "            \"first_name\": name_info.get(\"given\", [\"\"])[0],\n",
    "            \"last_name\": name_info.get(\"family\", \"\"),\n",
    "            \"prefix\": name_info.get(\"prefix\", [\"\"])[0] if name_info.get(\"prefix\") else None,\n",
    "            \"gender\": resource.get(\"gender\"),\n",
    "            #\"birth_date\": datetime.fromisoformat(resource.get(\"birthDate\")) if resource.get(\"birthDate\") else None,\n",
    "            \"npi\": npi,\n",
    "            \"license_number\": license_number,\n",
    "            \"primary_email\": next((t.get(\"value\") for t in resource.get(\"telecom\", []) if t.get(\"system\") == \"email\"), None),\n",
    "            \"primary_phone\": next((t.get(\"value\") for t in resource.get(\"telecom\", []) if t.get(\"system\") == \"phone\"), None),\n",
    "            \"load_timestamp\": datetime.now(timezone.utc)\n",
    "        })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Main ETL loop\n",
    "def etl_practitioners():\n",
    "    try:\n",
    "        for batch in fetch_staged_data(\"practitioners_fhir_raw\"):\n",
    "            df = transform_practitioners(batch)\n",
    "            if not df.empty:\n",
    "                insert_to_bq(df, \"practitioners\")\n",
    "                print(\"***Inserted batch***\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in practitioners ETL: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    etl_practitioners()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22ad0b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208D00000X\n",
      "General Practice Physician\n",
      "General Practice Physician\n",
      "208D00000X\n",
      "208D00000X\n",
      "General Practice Physician\n",
      "General Practice Physician\n",
      "208D00000X\n",
      "208D00000X\n",
      "General Practice Physician\n",
      "General Practice Physician\n",
      "208D00000X\n",
      "208D00000X\n",
      "General Practice Physician\n",
      "General Practice Physician\n",
      "208D00000X\n",
      "208D00000X\n",
      "General Practice Physician\n",
      "General Practice Physician\n",
      "208D00000X\n",
      "***Inserted batch***\n"
     ]
    }
   ],
   "source": [
    "#---Practitioner Roles ETL\n",
    "import psycopg2\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "# Path to your service account JSON key\n",
    "key_path = \"/Users/toniventura/keys/bq_key.json\"\n",
    "\n",
    "# Create credentials and BigQuery client\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "BQ_PROJECT = \"fhir-synthea-data\"\n",
    "BQ_DATASET = \"fhir_curated\"\n",
    "client = bigquery.Client(project=BQ_PROJECT, credentials=credentials)\n",
    "\n",
    "# Postgres config\n",
    "PG_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"fhir\",\n",
    "    \"user\": \"toniventura\",\n",
    "    \"password\": \"fhir_project\"\n",
    "}\n",
    "\n",
    "def insert_to_bq(df, table_name):\n",
    "    table_id = f\"{BQ_PROJECT}.{BQ_DATASET}.{table_name}\"\n",
    "    job = client.load_table_from_dataframe(df, table_id)\n",
    "    job.result()\n",
    "\n",
    "# Transform Practitioners\n",
    "def transform_practitioner_roles(rows):\n",
    "    records = []\n",
    "    for r in rows:\n",
    "        rid, resource = r[1], r[2]\n",
    "\n",
    "        # Initialize columns\n",
    "        specialty_text = resource.get(\"specialty\", {})[0].get(\"text\")\n",
    "        #specialty_code = resource.get(\"specialty\", {})[0].get(\"code\",[{}][0].get(\"coding\",[{}][0].get(\"code\")))\n",
    "        specialty_code = resource.get(\"specialty\", [{}])[0].get(\"coding\", [{}])[0].get(\"code\")\n",
    "        role_text = resource.get(\"code\", [{}])[0].get(\"text\")\n",
    "        role_code = resource.get(\"code\", [{}])[0].get(\"coding\", [{}])[0].get(\"code\")\n",
    "        \n",
    "        print(specialty_code)\n",
    "        print(specialty_text)\n",
    "        print(role_text)\n",
    "        print(role_code)\n",
    "\n",
    "        \n",
    "\n",
    "        records.append({\n",
    "            \"practitioner_role_id\": rid,\n",
    "            \"practitioner_npi\": resource.get(\"practitioner\", {}).get(\"identifier\").get(\"value\"),\n",
    "            \"organization_id\": resource.get(\"organization\", {}).get(\"identifier\").get(\"value\"),\n",
    "            \"specialty_code\": specialty_code,\n",
    "            \"specialty_text\": specialty_text,\n",
    "            \"role_text\" : role_text, \n",
    "            \"role_code\": role_code, \n",
    "            #                 if resource.get(\"telecom\") and resource[\"telecom\"][0].get(\"system\")==\"email\" else None,\n",
    "            #\"role_text\": resource.get(\"telecom\", [{}])[0].get(\"value\") \n",
    "                              #if resource.get(\"telecom\") and resource[\"telecom\"][0].get(\"system\")==\"phone\" else None,\n",
    "            \"load_timestamp\": datetime.now(timezone.utc)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "# Main ETL loop\n",
    "def etl_practitioner_roles():\n",
    "    try:\n",
    "        for batch in fetch_staged_data(\"practitioner_roles_fhir_raw\"):\n",
    "            df = transform_practitioner_roles(batch)\n",
    "            if not df.empty:\n",
    "                insert_to_bq(df, \"practitioner_roles\")\n",
    "                print(\"***Inserted batch***\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in practitioner roles ETL: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    etl_practitioner_roles()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54af36d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "# Path to your service account JSON key\n",
    "key_path = \"/Users/toniventura/keys/bq_key.json\"  # \n",
    "\n",
    "# Create credentials and BigQuery client\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "BQ_PROJECT = \"fhir-synthea-data\"\n",
    "BQ_DATASET = \"fhir_curated\"\n",
    "client = bigquery.Client(project=BQ_PROJECT, credentials=credentials)\n",
    "\n",
    "# Create dataset if it doesn't exist\n",
    "dataset_ref = bigquery.Dataset(f\"{BQ_PROJECT}.{BQ_DATASET}\")\n",
    "try:\n",
    "    client.get_dataset(dataset_ref)  # Check if dataset exists\n",
    "    print(f\"Dataset '{BQ_DATASET}' already exists.\")\n",
    "except:\n",
    "    dataset = bigquery.Dataset(dataset_ref)\n",
    "    dataset.location = \"US\"  # Choose your region\n",
    "    client.create_dataset(dataset)\n",
    "    print(f\"Dataset '{BQ_DATASET}' created.\")\n",
    "\n",
    "# 4️Define table schema\n",
    "table_id = f\"{BQ_PROJECT}.{BQ_DATASET}.patients\"\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"patient_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"first_name\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"last_name\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"birth_date\", \"DATE\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"gender\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"load_timestamp\", \"TIMESTAMP\", mode=\"REQUIRED\")\n",
    "]\n",
    "\n",
    "# Create table if it doesn't exist\n",
    "try:\n",
    "    client.get_table(table_id)\n",
    "    print(f\"Table 'patients' already exists.\")\n",
    "except:\n",
    "    table = bigquery.Table(table_id, schema=schema)\n",
    "    client.create_table(table)\n",
    "    print(f\"Table 'patients' created.\")\n",
    "\n",
    "# Insert a sample row\n",
    "rows_to_insert = [\n",
    "    {\n",
    "        \"patient_id\": \"p001\",\n",
    "        \"first_name\": \"John\",\n",
    "        \"last_name\": \"Doe\",\n",
    "        \"birth_date\": \"1980-01-01\",\n",
    "        \"gender\": \"M\",\n",
    "        \"load_timestamp\": datetime.utcnow().isoformat()\n",
    "    }\n",
    "]\n",
    "\n",
    "errors = client.insert_rows_json(table_id, rows_to_insert)\n",
    "if errors:\n",
    "    print(\"Encountered errors while inserting rows:\", errors)\n",
    "else:\n",
    "    print(\"Sample row inserted successfully.\")\n",
    "\n",
    "# 7️⃣ Query the table to verify\n",
    "query = f\"SELECT * FROM `{table_id}` LIMIT 5\"\n",
    "df = client.query(query).to_dataframe()\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
